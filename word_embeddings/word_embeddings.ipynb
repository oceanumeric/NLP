{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embeddings with Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "import emoji\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dict(data):\n",
    "    \"\"\"\n",
    "    Input: data a list of word indices\n",
    "    Output: word dict\n",
    "    \"\"\"\n",
    "    words = sorted(list(set(data)))\n",
    "    n = len(words)\n",
    "    idx = 0\n",
    "    \n",
    "    word2ind = {}\n",
    "    ind2word = {}\n",
    "    for k in words:\n",
    "        word2ind[k] = idx\n",
    "        ind2word[idx] = k\n",
    "        idx += 1\n",
    "    \n",
    "    return word2ind, ind2word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning and Tokenizing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = 'Who ❤️ \"word embeddings\" in 2022? I do!!!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Who ❤️ \"word embeddings\" in 2022. I do.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = re.sub(r'[,!?;-]+', '.', corpus)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after the tokenization:['Who', '❤️', '``', 'word', 'embeddings', \"''\", 'in', '2022', '.', 'I', 'do', '.']\n"
     ]
    }
   ],
   "source": [
    "data = word_tokenize(data)\n",
    "print(f\"after the tokenization:{data}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/6q/6fqz6gzx4hl547b9mv0n6q380000gn/T/ipykernel_97641/2761875446.py:3: DeprecationWarning: 'emoji.get_emoji_regexp()' is deprecated and will be removed in version 2.0.0. If you want to remove emoji from a string, consider the method emoji.replace_emoji(str, replace='').\n",
      "To hide this warning, pin/downgrade the package to 'emoji~=1.6.3'\n",
      "  if x.isalpha() or x == '.' or emoji.get_emoji_regexp().search(x)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['who', '❤️', 'word', 'embeddings', 'in', '.', 'i', 'do', '.']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# clean the token\n",
    "data = [x.lower() for x in data\n",
    "        if x.isalpha() or x == '.' or emoji.get_emoji_regexp().search(x)]\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(corpus):\n",
    "    data = re.sub(r'[,!?;-]+', '.', corpus)\n",
    "    data = word_tokenize(data)\n",
    "    data = [x.lower() for x in data\n",
    "        if x.isalpha() or x == '.' or emoji.get_emoji_regexp().search(x)]\n",
    "    \n",
    "    return data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sliding window of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_windows(words, window_size):\n",
    "    i = window_size\n",
    "    while i < len(words)-window_size:\n",
    "        center = words[i]\n",
    "        context_words = words[(i-window_size):i] + words[(i+1):(i+window_size+1)]\n",
    "        yield context_words, center\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'am', 'because', 'i']\thappy\n",
      "['am', 'happy', 'i', 'am']\tbecause\n",
      "['happy', 'because', 'am', 'learning']\ti\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "for x, y in get_windows(['i', 'am', 'happy', 'because', 'i', 'am', 'learning'], 2):\n",
    "    print(f'{x}\\t{y}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['now', 'your']\tit\n",
      "['it', 'turn']\tyour\n",
      "['your', 'try']\tturn\n",
      "['turn', 'with']\ttry\n",
      "['try', 'your']\twith\n",
      "['with', 'own']\tyour\n",
      "['your', 'sentence']\town\n",
      "['own', '.']\tsentence\n",
      "['sentence', 'this']\t.\n",
      "['.', 'is']\tthis\n",
      "['this', 'the']\tis\n",
      "['is', 'last']\tthe\n",
      "['the', 'time']\tlast\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/6q/6fqz6gzx4hl547b9mv0n6q380000gn/T/ipykernel_97641/3780535467.py:5: DeprecationWarning: 'emoji.get_emoji_regexp()' is deprecated and will be removed in version 2.0.0. If you want to remove emoji from a string, consider the method emoji.replace_emoji(str, replace='').\n",
      "To hide this warning, pin/downgrade the package to 'emoji~=1.6.3'\n",
      "  if x.isalpha() or x == '.' or emoji.get_emoji_regexp().search(x)]\n"
     ]
    }
   ],
   "source": [
    "for x, y in get_windows(tokenize(\"Now it's your turn: try with your own sentence! This is the last time\"), 1):\n",
    "    print(f'{x}\\t{y}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transforming words into vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'am', 'happy', 'because', 'i', 'am', 'learning']\n",
      "{'am': 0, 'because': 1, 'happy': 2, 'i': 3, 'learning': 4}\n"
     ]
    }
   ],
   "source": [
    "words = 'I am happy because I am learning'\n",
    "test_token= tokenize(words)\n",
    "print(test_token)\n",
    "word2ind, ind2word = get_dict(test_token)  # sorted and indexed\n",
    "print(word2ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 1., 0., 0.])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "center_word_vector = np.zeros(len(word2ind))\n",
    "happy_idx = word2ind['happy']\n",
    "center_word_vector[happy_idx] = 1 \n",
    "center_word_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_vector(word, word2ind):\n",
    "    vector = np.zeros(len(word2ind))\n",
    "    vector[word2ind[word]] = 1\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 1.])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_vector('learning', word2ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0., 0., 0., 1., 0.]),\n",
       " array([1., 0., 0., 0., 0.]),\n",
       " array([0., 1., 0., 0., 0.]),\n",
       " array([0., 0., 0., 1., 0.])]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# center word: happy\n",
    "# context word: ['i', 'am', 'because', 'i']\n",
    "context_words = ['i', 'am', 'because', 'i']\n",
    "context_vector = [one_hot_vector(x, word2ind) for x in context_words]\n",
    "context_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.25, 0.25, 0.  , 0.5 , 0.  ])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(context_vector, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def context_words_to_vector(context_words, word2ind):\n",
    "    context_vector = [one_hot_vector(x, word2ind) for x in context_words]\n",
    "    return np.mean(context_vector, axis=0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.25, 0.25, 0.  , 0.5 , 0.  ])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test\n",
    "context_words_to_vector(['i', 'am', 'because', 'i'], word2ind)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A very simple training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I am happy because I am learning'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'am', 'happy', 'because', 'i', 'am', 'learning']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_token = tokenize(words)\n",
    "words_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context words:  ['i', 'am', 'because', 'i'] -> [0.25 0.25 0.   0.5  0.  ]\n",
      "Center word:  happy -> [0. 0. 1. 0. 0.]\n",
      "\n",
      "Context words:  ['am', 'happy', 'i', 'am'] -> [0.5  0.   0.25 0.25 0.  ]\n",
      "Center word:  because -> [0. 1. 0. 0. 0.]\n",
      "\n",
      "Context words:  ['happy', 'because', 'am', 'learning'] -> [0.25 0.25 0.25 0.   0.25]\n",
      "Center word:  i -> [0. 0. 0. 1. 0.]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for context_words, center_word in get_windows(words_token, 2):\n",
    "    print(f'Context words:  {context_words} -> {context_words_to_vector(context_words, word2ind)}')\n",
    "    print(f'Center word:  {center_word} -> {one_hot_vector(center_word, word2ind)}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_example(words, c, word2ind):\n",
    "    for context_words, center_word in get_windows(words, c):\n",
    "        yield context_words_to_vector(context_words, word2ind), one_hot_vector(center_word, word2ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context words vector:  [0.25 0.25 0.   0.5  0.  ]\n",
      "Center word vector:  [0. 0. 1. 0. 0.]\n",
      "\n",
      "Context words vector:  [0.5  0.   0.25 0.25 0.  ]\n",
      "Center word vector:  [0. 1. 0. 0. 0.]\n",
      "\n",
      "Context words vector:  [0.25 0.25 0.25 0.   0.25]\n",
      "Center word vector:  [0. 0. 0. 1. 0.]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print vectors associated to center and context words for corpus using the generator function\n",
    "for context_words_vector, center_word_vector in get_training_example(words_token, 2, word2ind):\n",
    "    print(f'Context words vector:  {context_words_vector}')\n",
    "    print(f'Center word vector:  {center_word_vector}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 1) [[-3.13199432]\n",
      " [ 3.15124387]\n",
      " [-1.37668968]\n",
      " [-3.36197041]\n",
      " [ 2.3675512 ]]\n"
     ]
    }
   ],
   "source": [
    "# we have 'i am happy because i am learning'\n",
    "# vocabulary size = 5\n",
    "# z1 = w1 x + b1 w1: \n",
    "z1 = 10 * np.random.rand(5, 1) - 5\n",
    "print(z1.shape, z1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        ],\n",
       "       [3.15124387],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [2.3675512 ]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h = z1.copy()\n",
    "h[h<0] = 0 \n",
    "h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(z):\n",
    "    result = z.copy()\n",
    "    result[result<0] = 0\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        ],\n",
       "       [4.50714306],\n",
       "       [2.31993942],\n",
       "       [0.98658484],\n",
       "       [0.        ]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define a new vector and save it in the 'z' variable\n",
    "z = np.array([[-1.25459881], [ 4.50714306], [ 2.31993942], [ 0.98658484], [-3.4398136 ]])\n",
    "\n",
    "# Apply ReLU to it\n",
    "relu(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(z):\n",
    "    e_z = np.exp(z)\n",
    "    sum_ez = np.sum(e_z)\n",
    "    return e_z/sum_ez"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.08276948, 0.03044919, 0.61158833, 0.22499077, 0.05020223])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax([9, 8, 11, 10, 8.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(softmax([9, 8, 11, 10, 8.5])) == 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix the size of the word embedding vector\n",
    "N = 3\n",
    "V = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.26753195, 0.58445581, 0.98514387, 0.11956682, 0.57574174],\n",
       "       [0.44381793, 0.75855231, 0.17901368, 0.8219056 , 0.73204009],\n",
       "       [0.66595379, 0.55649323, 0.18355884, 0.14809481, 0.22912759]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# z = w1x + b, x.shape = 5x1, z.shape = 3x1, then w1 = 3x5\n",
    "# y = w2h + b2, h.shape = 3x1, w2 = 5x3\n",
    "W1 = np.random.rand(3, 5)\n",
    "W1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.84014581],\n",
       "       [0.75432424],\n",
       "       [0.35306756]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b1 = np.random.rand(3, 1)\n",
    "b1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.19538232, 0.80931918, 0.10330246],\n",
       "       [0.3506357 , 0.20557748, 0.45469874],\n",
       "       [0.50281739, 0.04734632, 0.73640318],\n",
       "       [0.63200585, 0.18889334, 0.86938257],\n",
       "       [0.95444699, 0.77347653, 0.30710177]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W2 = np.random.rand(5, 3)\n",
    "W2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.04265582],\n",
       "       [0.02695106],\n",
       "       [0.68063716],\n",
       "       [0.80160903],\n",
       "       [0.8633817 ]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b2 = np.random.rand(5, 1)\n",
    "b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'am', 'happy', 'because', 'i', 'am', 'learning']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'am': 0, 'because': 1, 'happy': 2, 'i': 3, 'learning': 4} {0: 'am', 1: 'because', 2: 'happy', 3: 'i', 4: 'learning'}\n"
     ]
    }
   ],
   "source": [
    "print(word2ind, ind2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_examples = get_training_example(words_token, 2, word2ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.25 0.25 0.   0.5  0.  ] [0. 0. 1. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "x_array, y_array = next(training_examples)\n",
    "print(x_array, y_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x:\n",
      " [[0.25]\n",
      " [0.25]\n",
      " [0.  ]\n",
      " [0.5 ]\n",
      " [0.  ]] \n",
      " y:\n",
      " [[0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]]\n"
     ]
    }
   ],
   "source": [
    "# reshape the vector\n",
    "x = x_array.copy()\n",
    "x.shape = (V, 1)\n",
    "y = y_array.copy()\n",
    "y.shape = (V, 1)\n",
    "print('x:\\n', x, '\\n', 'y:\\n', y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.27278035],\n",
       "       [0.71154536],\n",
       "       [0.37965916]])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W1 @ x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.27278035],\n",
       "       [0.71154536],\n",
       "       [0.37965916]])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# np.dot is also matrix multiplication\n",
    "np.dot(W1, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.11292616],\n",
       "       [1.4658696 ],\n",
       "       [0.73272672]])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z1 = W1 @ x + b1\n",
    "z1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.11292616],\n",
       "       [1.4658696 ],\n",
       "       [0.73272672]])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h = relu(z1)\n",
    "h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.52215077],\n",
       "       [1.0517024 ],\n",
       "       [1.8492216 ],\n",
       "       [2.41889773],\n",
       "       [3.28444813]])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z2 = W2 @ h + b2\n",
    "z2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.08858147],\n",
       "       [0.0553388 ],\n",
       "       [0.12285362],\n",
       "       [0.21716765],\n",
       "       [0.51605846]])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat = softmax(z2)\n",
    "y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'learning'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ind2word[np.argmax(y_hat)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_loss(y_predicted, y_actual):\n",
    "    loss = np.sum(-np.log(y_predicted)*y_actual)  # element wise multiplication \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.096761752309792"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_entropy_loss(y_hat, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.08858147],\n",
       "       [ 0.0553388 ],\n",
       "       [-0.87714638],\n",
       "       [ 0.21716765],\n",
       "       [ 0.51605846]])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad_b2 = y_hat -y\n",
    "grad_b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.09858464,  0.12984888,  0.06490601],\n",
       "       [ 0.061588  ,  0.08111947,  0.04054822],\n",
       "       [-0.97619916, -1.28578222, -0.64270859],\n",
       "       [ 0.24169156,  0.31833946,  0.15912454],\n",
       "       [ 0.57433496,  0.75647441,  0.37812982]])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad_w2 = np.dot(y_hat - y, h.T)\n",
    "grad_w2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.22546823],\n",
       "       [0.48171807],\n",
       "       [0.        ]])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad_b1 = relu(np.dot(W2.T, y_hat - y))\n",
    "grad_b1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.05636706, 0.05636706, 0.        , 0.11273412, 0.        ],\n",
       "       [0.12042952, 0.12042952, 0.        , 0.24085904, 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ]])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad_w1 = np.dot(relu(np.dot(W2.T, y_hat-y)), x.T)\n",
    "grad_w1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning rate\n",
    "alpha = 0.03"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.05636706 0.05636706 0.         0.11273412 0.        ]\n",
      " [0.12042952 0.12042952 0.         0.24085904 0.        ]\n",
      " [0.         0.         0.         0.         0.        ]] \n",
      " [[0.26584094 0.5827648  0.98514387 0.11618479 0.57574174]\n",
      " [0.44020505 0.75493943 0.17901368 0.81467983 0.73204009]\n",
      " [0.66595379 0.55649323 0.18355884 0.14809481 0.22912759]]\n"
     ]
    }
   ],
   "source": [
    "w1_update = W1 - alpha * grad_w1\n",
    "print(grad_w1, '\\n', w1_update)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3b30f1e37ab6bb31f04d49272996ab097d78250cb126a8dec2a7497893895f41"
  },
  "kernelspec": {
   "display_name": "Python 3.9.2 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
