{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Network (RNN) from scratch\n",
    "\n",
    "This notebook again is based on the [blog post](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) by Andrej Karpathy. The goal is to implement a simple RNN from scratch in Python and train it to perform character-level language modeling.\n",
    "\n",
    "Andrej also gave a talk about RNNs at the Deep Learning Summer School 2015, which is available [here](https://skillsmatter.com/skillscasts/6611-visualizing-and-understanding-recurrent-networks#video).\n",
    "\n",
    "Instead of using numpy, we will use pytorch to implement the RNN. This will allow us to easily run the code on a GPU.\n",
    "\n",
    "Here, we will use Shakespeare's Sonnets as the training data. We are not making names because the sequence of names is not very long and the RNN will not be able to learn much from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages that are not related to torch\n",
    "import os\n",
    "import math\n",
    "import time\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# torch import\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as tu_data\n",
    "from torchvision.datasets import FashionMNIST\n",
    "\n",
    "\n",
    "### --------- environment setup --------- ###\n",
    "# set up the data path\n",
    "DATA_PATH = \"../GPT-2/data\"\n",
    "\n",
    "# function for setting seed\n",
    "def set_seed(seed):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        \n",
    "# set up seed globally and deterministically\n",
    "set_seed(42)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# set up device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the dataset (names)\n",
    "shk_text = open(os.path.join(DATA_PATH, \"input.txt\"), \"r\").read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You\n"
     ]
    }
   ],
   "source": [
    "print(shk_text[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n', ' ', '!', '$', '&', \"'\", ',', '-', '.', '3', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
      "The number of unique characters: 65\n",
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n"
     ]
    }
   ],
   "source": [
    "# we will work on characters\n",
    "# we will use '.' as the start and end token\n",
    "chars = sorted(list(set(shk_text)))\n",
    "print(chars)\n",
    "print(\"The number of unique characters: {}\".format(len(chars)))\n",
    "print(\"\".join(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'\\n': 0, ' ': 1, '!': 2, '$': 3, '&': 4, \"'\": 5, ',': 6, '-': 7, '.': 8, '3': 9, ':': 10, ';': 11, '?': 12, 'A': 13, 'B': 14, 'C': 15, 'D': 16, 'E': 17, 'F': 18, 'G': 19, 'H': 20, 'I': 21, 'J': 22, 'K': 23, 'L': 24, 'M': 25, 'N': 26, 'O': 27, 'P': 28, 'Q': 29, 'R': 30, 'S': 31, 'T': 32, 'U': 33, 'V': 34, 'W': 35, 'X': 36, 'Y': 37, 'Z': 38, 'a': 39, 'b': 40, 'c': 41, 'd': 42, 'e': 43, 'f': 44, 'g': 45, 'h': 46, 'i': 47, 'j': 48, 'k': 49, 'l': 50, 'm': 51, 'n': 52, 'o': 53, 'p': 54, 'q': 55, 'r': 56, 's': 57, 't': 58, 'u': 59, 'v': 60, 'w': 61, 'x': 62, 'y': 63, 'z': 64}\n",
      "{0: '\\n', 1: ' ', 2: '!', 3: '$', 4: '&', 5: \"'\", 6: ',', 7: '-', 8: '.', 9: '3', 10: ':', 11: ';', 12: '?', 13: 'A', 14: 'B', 15: 'C', 16: 'D', 17: 'E', 18: 'F', 19: 'G', 20: 'H', 21: 'I', 22: 'J', 23: 'K', 24: 'L', 25: 'M', 26: 'N', 27: 'O', 28: 'P', 29: 'Q', 30: 'R', 31: 'S', 32: 'T', 33: 'U', 34: 'V', 35: 'W', 36: 'X', 37: 'Y', 38: 'Z', 39: 'a', 40: 'b', 41: 'c', 42: 'd', 43: 'e', 44: 'f', 45: 'g', 46: 'h', 47: 'i', 48: 'j', 49: 'k', 50: 'l', 51: 'm', 52: 'n', 53: 'o', 54: 'p', 55: 'q', 56: 'r', 57: 's', 58: 't', 59: 'u', 60: 'v', 61: 'w', 62: 'x', 63: 'y', 64: 'z'}\n"
     ]
    }
   ],
   "source": [
    "# create index\n",
    "char2idx = {ch: i for i, ch in enumerate(chars)}\n",
    "idx2char = {i: ch for i, ch in enumerate(chars)}\n",
    "print(char2idx)\n",
    "print(idx2char)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start, let's just use one function to understand the basic idea of RNNs: \n",
    "\n",
    "$$\n",
    "h_{t+1}  = tanh(W_{hh} h_t + W_{xh} x_t)\n",
    "$$\n",
    "\n",
    "where $h_{t+1}$ is the hidden layer. After the hidden layer we have\n",
    "the output layer and then softmax to get the probabilities for the next character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The text has 1115394 characters, 65 unique.\n"
     ]
    }
   ],
   "source": [
    "# set up hyperparameters\n",
    "text_size, vocab_size = len(shk_text), len(chars)\n",
    "print(\"The text has {} characters, {} unique.\".format(text_size, vocab_size))\n",
    "hidden_size = 100 \n",
    "seq_len = 9  # the length of the sequence\n",
    "learning_rate = 1e-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of parameters: 23165\n"
     ]
    }
   ],
   "source": [
    "# initialize the parameters\n",
    "# since we are doing one-hot encoding,\n",
    "# the input size is the same as the vocab_size\n",
    "wxh = torch.randn(hidden_size, vocab_size,\n",
    "                  device=device, dtype=torch.float32,\n",
    "                  requires_grad=True)\n",
    "whh = torch.randn(hidden_size, hidden_size,\n",
    "                    device=device, dtype=torch.float32,\n",
    "                    requires_grad=True)\n",
    "why = torch.randn(vocab_size, hidden_size,\n",
    "                    device=device, dtype=torch.float32,\n",
    "                    requires_grad=True)\n",
    "# bias\n",
    "bh = torch.zeros(hidden_size, device=device,\n",
    "                    dtype=torch.float32, requires_grad=True)\n",
    "by = torch.zeros(vocab_size, device=device,\n",
    "                    dtype=torch.float32, requires_grad=True)\n",
    "parameters = [wxh, whh, why, bh, by]\n",
    "# print out the number of parameters\n",
    "print(\"The number of parameters: {}\".format(sum(p.numel() for p in parameters)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[46, 43, 50, 50, 53]\n",
      "hello\n"
     ]
    }
   ],
   "source": [
    "# prepare the data\n",
    "encode = lambda text: [char2idx[ch] for ch in text]\n",
    "decode = lambda tnsr: \"\".join([idx2char[i] for i in tnsr])\n",
    "\n",
    "# test the encode and decode functions\n",
    "print(encode(\"hello\"))\n",
    "print(decode(encode(\"hello\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The text has 1115394 characters, 65 unique.\n",
      "torch.Size([1115394])\n",
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# we now will encode the whole text\n",
    "encoded_text = torch.tensor(encode(shk_text), device=device, dtype=torch.long)\n",
    "print(\"The text has {} characters, {} unique.\".format(text_size, vocab_size))\n",
    "# encoded_text is a 1D tensor and has the length of the text\n",
    "print(encoded_text.shape)\n",
    "print(encoded_text[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of training data: 1003854\n",
      "The number of test data: 111540\n"
     ]
    }
   ],
   "source": [
    "# now split the dataset into training and validation\n",
    "# split the dataset into train and test sets\n",
    "train_n = int(encoded_text.shape[0] * 0.9)\n",
    "train_data = encoded_text[:train_n]\n",
    "test_data = encoded_text[train_n:]\n",
    "print(\"The number of training data: {}\".format(train_data.shape[0]))\n",
    "print(\"The number of test data: {}\".format(test_data.shape[0]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structure of training a nueral network\n",
    "\n",
    "What are the key components of training a neural network?\n",
    "\n",
    "- load the original data\n",
    "- preprocess the data\n",
    "- initialize the parameters\n",
    "- forward propagation\n",
    "- compute the loss\n",
    "- backpropagation\n",
    "- update the parameters\n",
    "- predict\n",
    "\n",
    "Working with text, reading the dataset is not difficult. We just need to read the text file and convert it to a list of characters or tokens. However, once we have the data, we need to preprocess it. This is a very important step. We need to convert the characters to numbers. We also need to create a dictionary to map the numbers back to the characters. The common way to do this is to use one-hot encoding, which needs to construct __a dictionary__ to map the characters to numbers.\n",
    "\n",
    "Once we have a dictionary, we can use it to convert the characters to integers. However, this one-hot encoding is not very efficient. We will use a different way to encode the characters. We will use the `Embedding` layer in pytorch to do this. This layer will map the characters to a vector of real numbers. This is a more efficient way to encode the characters. Therefore, here are common steps to preprocess the data:\n",
    "\n",
    "- read the text file\n",
    "- clean the text on character level or token level\n",
    "- construct a dictionary to map the characters to numbers\n",
    "- then characters flow into the neural network as numbers\n",
    "    - human being read the text as characters\n",
    "    - the neural network read the text as numbers\n",
    "\n",
    "Once, we have the data, we need to initialize the parameters. Without using `Pytorch`, one has to initialize the parameters manually. However, with `Pytorch`, we can use the `nn` module to do this. We will use the `nn` module to initialize the parameters. Or if the modules are not available, we can write our own `class` to initialize the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now construct the embedding layer\n",
    "# we do not need to set up the device as we coudl use the .to(device) method\n",
    "class Embedding:\n",
    "  \n",
    "  def __init__(self, dict_size, embedding_dim):\n",
    "    \"\"\"\n",
    "    dict_size: the size of the dictionary\n",
    "    embedding_dim: the dimension of the embedding\n",
    "    \"\"\"\n",
    "    self.weight = torch.randn((dict_size, embedding_dim),\n",
    "                                              device=device,\n",
    "                                              dtype=torch.float32,\n",
    "                                              requires_grad=True)\n",
    "    \n",
    "  def __call__(self, IX):\n",
    "    \"\"\"\n",
    "    embedding layer will be the first layer of the network\n",
    "    and each time we will pass a batch of data to the network\n",
    "    the input.shape = (batch_size, seq_len), then \n",
    "    the output.shape = (batch_size, seq_len, embedding_dim)\n",
    "    the input datatype has to be int, such as torch.long\n",
    "    This way we could train the embedding layer efficiently\n",
    "    batch by batch instead of training the whole dataset\n",
    "    \"\"\"\n",
    "    self.out = self.weight[IX]\n",
    "    return self.out\n",
    "  \n",
    "  def parameters(self):\n",
    "    return [self.weight]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# since RNN rolls out the sequence, we need to define the RNNCell\n",
    "# based on https://pytorch.org/docs/stable/generated/torch.nn.RNNCell.html?highlight=rnncell\n",
    "# you can watch this for more details: https://youtu.be/ySEx_Bqxvvo\n",
    "class RNNCell:\n",
    "\n",
    "    def __init__(self, embedding_size, hidden_size, output_size, bias=True):\n",
    "        \"\"\"\n",
    "        input_size: the size of the input\n",
    "        hidden_size: the size of the hidden state\n",
    "        remark: the order matters, which determines whether we should\n",
    "        call input @ weight or weight @ input\n",
    "        here we set up the weight as (input_size, hidden_size)\n",
    "        therefore, we should call input @ weight\n",
    "        \"\"\"\n",
    "        self.vocab_size = output_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.wxh = torch.randn((embedding_size, hidden_size), device=device,\n",
    "                                                    dtype=torch.float32,\n",
    "                                                    requires_grad=True)\n",
    "        self.whh = torch.randn((hidden_size, hidden_size), device=device,\n",
    "                                                    dtype=torch.float32,\n",
    "                                                    requires_grad=True)\n",
    "        self.why = torch.randn((hidden_size, vocab_size), device=device,\n",
    "                                                    dtype=torch.float32,\n",
    "                                                    requires_grad=True)\n",
    "        if bias:\n",
    "            self.bh = torch.zeros((1, hidden_size), device=device,\n",
    "                                                    dtype=torch.float32,\n",
    "                                                    requires_grad=True)\n",
    "        else:\n",
    "            self.bh = None\n",
    "\n",
    "    def __call__(self, x, h):\n",
    "        \"\"\"\n",
    "        x: the input of the RNNCell\n",
    "               the shape of the input x.shape = (batch_size, vocab_size)\n",
    "               there is no seq_len dimension\n",
    "        h: the hidden state of the RNNCell, h.shape = (batch_size, hidden_size)\n",
    "        \"\"\"\n",
    "        if h is None:\n",
    "            # we need to initialize the hidden state\n",
    "            h = torch.zeros((x.shape[0], self.hidden_size), device=device,\n",
    "                                                    dtype=torch.float32,\n",
    "                                                    requires_grad=True)\n",
    "        self.hidden = torch.tanh(x @ self.wxh + h @ self.whh + self.bh)\n",
    "        self.out = self.hidden @ self.why\n",
    "        # return the output and the hidden state\n",
    "        return self.out, self.hidden\n",
    "    \n",
    "    def parameters(self):\n",
    "        return [self.wxh, self.whh, self.why, self.bh]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the second layer is the RNN layer\n",
    "# we will run a customer layer, __init__ part is the initialization\n",
    "# and the __call__ part is the forward pass\n",
    "# if the weight.shape = (input_size, hidden_size)\n",
    "# then in the forward pass, we should call weight @ input\n",
    "# otherwise, we should set up the weight as (hidden_size, input_size)\n",
    "# and call input @ weight\n",
    "# now we will create a RNN layer based on the RNNCell\n",
    "class RNN:\n",
    "\n",
    "    def __init__(self, rnn_cell):\n",
    "        \"\"\"\n",
    "        rnn_cell: the RNNCell\n",
    "        \"\"\"\n",
    "        self.rnn_cell = rnn_cell\n",
    "\n",
    "    \n",
    "    def __call__(self, X, h):\n",
    "        \"\"\"\n",
    "        X: the input of the RNN, X.shape = (batch_size, seq_length, embedding_dim)\n",
    "        h: the initial hidden state, h.shape = (batch_size, hidden_size)\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, embedding_dim = X.shape\n",
    "\n",
    "        outputs = []\n",
    "\n",
    "        for t in range(seq_len):\n",
    "            x_t = X[:, t, :]\n",
    "            out, h = self.rnn_cell(x_t, h)\n",
    "            outputs.append(out)\n",
    "        \n",
    "        # stack the outputs\n",
    "        outputs = torch.stack(outputs, dim=1)\n",
    "        return outputs, h\n",
    "    \n",
    "    def parameters(self):\n",
    "        return self.rnn_cell.parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the forward function\n",
    "def forward(X, Y, embedding, rnn, h):\n",
    "    \"\"\"\n",
    "    X: the input of the network, X.shape = (batch_size, seq_len)\n",
    "    Y: the target of the network, Y.shape = (batch_size, seq_len)\n",
    "    embedding: the embedding layer\n",
    "    rnn: the rnn layer\n",
    "    h: the initial hidden state\n",
    "    \"\"\"\n",
    "    # get the embedding\n",
    "    X = embedding(X)\n",
    "    # get the output and the hidden state\n",
    "    out, h = rnn(X, h)\n",
    "    # calculate the loss\n",
    "    # out.shape = (batch_size, seq_len, vocab_size)\n",
    "    # calculate the loss for each time step\n",
    "    # and then take the average\n",
    "    loss = F.cross_entropy(out.view(-1, out.shape[-1]), Y.view(-1)).mean()\n",
    "    \n",
    "    return loss, h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct X and Y\n",
    "def get_batch(encoded_text, seq_len, batch_size):\n",
    "    \"\"\"\n",
    "    encoded_text: the encoded text\n",
    "    seq_len: the length of the sequence\n",
    "    batch_size: the size of the batch\n",
    "    \"\"\"\n",
    "    # calculate the number of batches\n",
    "    n_batches = encoded_text.shape[0] // (seq_len * batch_size)\n",
    "    # reshape the encoded text\n",
    "    encoded_text = encoded_text[:n_batches * batch_size * seq_len]\n",
    "    encoded_text = encoded_text.reshape((batch_size, -1))\n",
    "    # loop through the encoded text\n",
    "    for i in range(0, encoded_text.shape[1], seq_len):\n",
    "        # get the input and the target\n",
    "        X = encoded_text[:, i:i+seq_len]\n",
    "        Y = torch.zeros_like(X)\n",
    "        Y[:, :-1], Y[:, -1] = X[:, 1:], X[:, 0]\n",
    "        yield X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test it\n",
    "foo_x, foo_y = next(get_batch(train_data, 10, 30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([30, 10]), torch.Size([30, 10]))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "foo_x.shape, foo_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47], device='cuda:0')\n",
      "tensor([47, 56, 57, 58,  1, 15, 47, 58, 47, 18], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(foo_x[0])\n",
    "print(foo_y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([30, 10, 65]) torch.Size([30, 10])\n",
      "torch.Size([300, 65]) torch.Size([300])\n"
     ]
    }
   ],
   "source": [
    "# we could draw batches randomly\n",
    "# but here we will draw batches sequentially\n",
    "# now we will go through the network\n",
    "\n",
    "foo_embedding = Embedding(vocab_size, 25)\n",
    "foo_embedding(foo_x).shape\n",
    "# second layer\n",
    "foo_rnn = RNN(RNNCell(25, 100, vocab_size))\n",
    "foo_out, foo_h = foo_rnn(foo_embedding(foo_x), None)\n",
    "print(foo_out.shape, foo_y.shape)\n",
    "print(foo_out.view(-1, foo_out.shape[-1]).shape, foo_y.view(-1).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(22.1778, device='cuda:0', grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the output is the shape of (batch_size, seq_len, vocab_size)\n",
    "# whereas Y.shape = torch.Size([30, 10])) which is (batch_size, seq_len)\n",
    "# we need to calculate the loss for each time step\n",
    "# and then average them\n",
    "F.cross_entropy(foo_out.view(-1, foo_out.shape[-1]), foo_y.view(-1)).mean()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.0836, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_foo = torch.randn(3, 5, requires_grad=True)\n",
    "target_foo = torch.randint(5, (3,), dtype=torch.int64)\n",
    "F.cross_entropy(input_foo, target_foo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0, loss: 31.93937110900879\n",
      "step: 1000, loss: 2.742612600326538\n",
      "step: 2000, loss: 2.7541823387145996\n",
      "step: 3000, loss: 2.7108449935913086\n",
      "step: 4000, loss: 2.6730246543884277\n",
      "step: 5000, loss: 2.644676923751831\n",
      "step: 6000, loss: 2.6060636043548584\n",
      "step: 7000, loss: 2.589829444885254\n",
      "step: 8000, loss: 2.580498456954956\n",
      "step: 9000, loss: 2.5517477989196777\n"
     ]
    }
   ],
   "source": [
    "# now, let's train the network\n",
    "max_steps = 10000\n",
    "batch_size = 600\n",
    "seq_len = 25\n",
    "embedding_dim = 65 # which is also the vocab_size\n",
    "hidden_size = 500\n",
    "lr = 1e-2\n",
    "\n",
    "# initialize the network\n",
    "embedding = Embedding(vocab_size, embedding_dim)\n",
    "rnn = RNN(RNNCell(embedding_dim, hidden_size, vocab_size))\n",
    "\n",
    "# initialize the optimizer\n",
    "optimizer = torch.optim.Adam([p for p in embedding.parameters()] + [p for p in rnn.parameters()], lr=lr)\n",
    "\n",
    "# begin to train the network\n",
    "for i in range(max_steps):\n",
    "\n",
    "    # get the batch\n",
    "    X, Y = next(get_batch(train_data, seq_len, batch_size))\n",
    "    # initialize the hidden state\n",
    "    h = None\n",
    "    # forward pass\n",
    "    loss, h = forward(X, Y, embedding, rnn, h)\n",
    "    # backward pass\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    # print the loss\n",
    "    if i % 1000 == 0:\n",
    "        print(f\"step: {i}, loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's generate some text\n",
    "def generate_text(embedding, rnn, h, seed_text, n_chars=20):\n",
    "    \"\"\"\n",
    "    We are now predicting the next character based on the previous text\n",
    "    embedding: the embedding layer has been trained\n",
    "    rnn: the rnn layer has been trained\n",
    "    h: the hidden state\n",
    "    seed_text: the seed text\n",
    "    We will use mutlinomial to sample the next character\n",
    "    \"\"\"\n",
    "\n",
    "    print(seed_text, end=\"\")\n",
    "\n",
    "    output_text = []\n",
    "\n",
    "    for i in range(n_chars):\n",
    "\n",
    "        # get the input\n",
    "        \n",
    "\n",
    "    \n",
    "    return \"\".join(output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tomorrow I will<class 'list'>\n",
      ":uO le  snsatl <class 'list'>\n",
      ",funarqsoW,ule<class 'list'>\n",
      "uudeunset!gll<class 'list'>\n",
      "u\n",
      "e iaecodha<class 'list'>\n",
      " .retotatel<class 'list'>\n",
      "eu,sogaal <class 'list'>\n",
      "nS iitel <class 'list'>\n",
      " w'noona<class 'list'>\n",
      "iesisl <class 'list'>\n",
      "mbHtee<class 'list'>\n",
      "mrll <class 'list'>\n",
      " se <class 'list'>\n",
      "gi <class 'list'>\n",
      " u<class 'int'>\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "object of type 'int' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2765382/1744158011.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# test it\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mgenerate_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrnn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Tomorrow I will\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_2765382/3812094128.py\u001b[0m in \u001b[0;36mgenerate_text\u001b[0;34m(embedding, rnn, h, seed_text, n_chars)\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mc_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mc_idx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m             \u001b[0;31m# print the character\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx2char\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mc_idx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'int' has no len()"
     ]
    }
   ],
   "source": [
    "# test it\n",
    "generate_text(embedding, rnn, None, \"Tomorrow I will\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlpvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
