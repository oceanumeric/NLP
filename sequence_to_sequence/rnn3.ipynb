{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Parameter\n",
    "\n",
    "# Check whether GPU is available and can be used\n",
    "# if CUDA is found then device is set accordingly\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "if not torch.cuda.is_available():\n",
    "    print(\"Consider changing your run-time to GPU or training will be slow.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From fairest creatures we desire increase,\n",
      "That thereby beauty's rose might never die,\n",
      "But as the riper should by time decease,\n",
      "His tender heir might bear his memory:\n",
      "But thou contracted to thine own \n"
     ]
    }
   ],
   "source": [
    "with open('data/sonnets.txt', 'r') as f:\n",
    "    text = f.read()\n",
    "    \n",
    "# print an excerpt of the text \n",
    "print(text[:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n', ' ', '!', \"'\", '(', ')', ',', '-', '.', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'R', 'S', 'T', 'U', 'V', 'W', 'Y', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
      "{'\\n': 0, ' ': 1, '!': 2, \"'\": 3, '(': 4, ')': 5, ',': 6, '-': 7, '.': 8, ':': 9, ';': 10, '?': 11, 'A': 12, 'B': 13, 'C': 14, 'D': 15, 'E': 16, 'F': 17, 'G': 18, 'H': 19, 'I': 20, 'J': 21, 'K': 22, 'L': 23, 'M': 24, 'N': 25, 'O': 26, 'P': 27, 'R': 28, 'S': 29, 'T': 30, 'U': 31, 'V': 32, 'W': 33, 'Y': 34, 'a': 35, 'b': 36, 'c': 37, 'd': 38, 'e': 39, 'f': 40, 'g': 41, 'h': 42, 'i': 43, 'j': 44, 'k': 45, 'l': 46, 'm': 47, 'n': 48, 'o': 49, 'p': 50, 'q': 51, 'r': 52, 's': 53, 't': 54, 'u': 55, 'v': 56, 'w': 57, 'x': 58, 'y': 59, 'z': 60}\n",
      "vocab length: 61\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([17, 52, 49, 47,  1, 40, 35, 43, 52, 39, 53, 54,  1, 37, 52, 39, 35,\n",
       "       54, 55, 52, 39, 53,  1, 57, 39,  1, 38, 39, 53, 43, 52, 39,  1, 43,\n",
       "       48, 37, 52, 39, 35, 53, 39,  6,  0, 30, 42, 35, 54,  1, 54, 42, 39,\n",
       "       52, 39, 36, 59,  1, 36, 39, 35, 55, 54, 59,  3, 53,  1, 52, 49, 53,\n",
       "       39,  1, 47, 43, 41, 42, 54,  1, 48, 39, 56, 39, 52,  1, 38, 43, 39,\n",
       "        6,  0, 13, 55, 54,  1, 35, 53,  1, 54, 42, 39,  1, 52, 43, 50, 39,\n",
       "       52,  1, 53, 42, 49, 55, 46, 38,  1, 36, 59,  1, 54, 43, 47, 39,  1,\n",
       "       38, 39, 37, 39, 35, 53, 39,  6,  0, 19, 43, 53,  1, 54, 39, 48, 38,\n",
       "       39, 52,  1, 42, 39, 43, 52,  1, 47, 43, 41, 42, 54,  1, 36, 39, 35,\n",
       "       52,  1, 42, 43, 53,  1, 47, 39, 47, 49, 52, 59,  9,  0, 13, 55, 54,\n",
       "        1, 54, 42, 49, 55,  1, 37, 49, 48, 54, 52, 35, 37, 54, 39, 38,  1,\n",
       "       54, 49,  1, 54, 42, 43, 48, 39,  1, 49, 57, 48,  1])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We create two dictionaries:\n",
    "# 1. int2char, which maps integers to characters\n",
    "# 2. char2int, which maps characters to integers\n",
    "chars = sorted(set(text))\n",
    "int2char = dict(enumerate(chars))\n",
    "char2int = {ch: ii for ii, ch in int2char.items()}\n",
    "\n",
    "print(chars)\n",
    "print(char2int)\n",
    "\n",
    "# Encode the text\n",
    "encoded = np.array([char2int[ch] for ch in text])\n",
    "\n",
    "print(f\"vocab length: {len(char2int)}\")\n",
    "# Again showing the excerpt, but this time as integers \n",
    "encoded[:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining method to make mini-batches for training\n",
    "def get_batches(arr, batch_size, seq_length):\n",
    "    # determine the flattened batch size, i.e. sequence length times batch size\n",
    "    batch_size_total = batch_size * seq_length\n",
    "    # total number of batches we can make\n",
    "    n_batches = len(arr)//batch_size_total\n",
    "    \n",
    "    # Keep only enough characters to make full batches\n",
    "    arr = arr[:n_batches * batch_size_total]\n",
    "    # Reshape into batch_size rows\n",
    "    arr = arr.reshape((batch_size, -1))\n",
    "    \n",
    "    # iterate through the array, one sequence at a time\n",
    "    for n in range(0, arr.shape[1], seq_length):\n",
    "        # The features\n",
    "        x = arr[:, n:n+seq_length]\n",
    "        # The targets\n",
    "        y = np.zeros_like(x)\n",
    "        try:\n",
    "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, n+seq_length]\n",
    "        except IndexError:\n",
    "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, 0]\n",
    "        yield x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 50) (10, 50)\n",
      "[[17 52 49 47  1 40 35 43 52 39]\n",
      " [52 39  1 36 46 39 53 53 39 38]\n",
      " [53 54 49 46  3 48  1 40 52 49]\n",
      " [52 54 10  0 12 48 38  1 36 59]\n",
      " [46 46  1 47 59  1 39 56 39 52]\n",
      " [39 48  1 48 55 52 53 39 38  6]\n",
      " [49 45 53  1 57 43 54 42  1 47]\n",
      " [56 39 52 59  1 53 35 47 39 10]\n",
      " [ 1 53 37 59 54 42 39  1 35 48]\n",
      " [39 46 53 39 57 42 39 52 39 10]]\n",
      "[[52 49 47  1 40 35 43 52 39 53]\n",
      " [39  1 36 46 39 53 53 39 38  1]\n",
      " [54 49 46  3 48  1 40 52 49 47]\n",
      " [54 10  0 12 48 38  1 36 59  1]\n",
      " [46  1 47 59  1 39 56 39 52 59]\n",
      " [48  1 48 55 52 53 39 38  6  1]\n",
      " [45 53  1 57 43 54 42  1 47 39]\n",
      " [39 52 59  1 53 35 47 39 10  0]\n",
      " [53 37 59 54 42 39  1 35 48 38]\n",
      " [46 53 39 57 42 39 52 39 10  1]]\n"
     ]
    }
   ],
   "source": [
    "foox, fooy = next(get_batches(encoded, 10, 50))\n",
    "print(foox.shape, fooy.shape)\n",
    "print(foox[:10, :10])\n",
    "print(fooy[:10, :10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(arr, n_labels):\n",
    "    \n",
    "    # Initialize the the encoded array\n",
    "    one_hot = np.zeros((np.multiply(*arr.shape), n_labels), dtype=np.float32)\n",
    "    \n",
    "    # Fill the appropriate elements with ones\n",
    "    one_hot[np.arange(one_hot.shape[0]), arr.flatten()] = 1.\n",
    "    \n",
    "    # Finally reshape it to get back to the original array\n",
    "    one_hot = one_hot.reshape((*arr.shape, n_labels))\n",
    "    \n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3987770715.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipykernel_4072719/3987770715.py\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    one_hot_encode(np.array([46 53 39 57 42 39 52 39 10  1]), 10)\u001b[0m\n\u001b[0m                                ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "one_hot_encode(np.array([[1, 2, 3]]), 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, chars, device, hidden_sz, drop_prob=0.5):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.device = device\n",
    "        \n",
    "        # creating character dictionaries\n",
    "        # we already have this code on the top, but giving it to our model \n",
    "        # will be convenient for doing predictions later\n",
    "        # i.e. doing conversions from text to integers to one-hot & vice-versa\n",
    "        self.n_chars = len(chars)\n",
    "        self.int2char = dict(enumerate(chars))\n",
    "        self.char2int = {ch: ii for ii, ch in self.int2char.items()}\n",
    "        \n",
    "        self.hidden_sz = hidden_sz\n",
    "        \n",
    "        # Note that this class inherits from the torch neural network class\n",
    "        # Instead of using a pre-built function we will write the math ourselves\n",
    "        # For this reason we will first need to define \"Parameters()\", that \n",
    "        # the PyTorch graph keeps track of and can optimize. In other words,\n",
    "        # let's give our class the weights & the bias that the RNN will need. \n",
    "        self.weight_ih = Parameter(torch.Tensor(self.n_chars, self.hidden_sz))\n",
    "        self.weight_hh = Parameter(torch.Tensor(self.hidden_sz, self.hidden_sz))\n",
    "        self.bias_hh = Parameter(torch.Tensor(self.hidden_sz))\n",
    "        \n",
    "        # Now that we have defined the RNN cell, let us define the output layer\n",
    "        # We will use a dropout layer to prevent overfitting and then \n",
    "        # follow with a conventional linear layer (matrix multiplication) that \n",
    "        # maps the RNN cell's output (the hidden state of the network) to the \n",
    "        # class output. Remembert that the class output corresponds to a \n",
    "        # vector of length of unique characters. \n",
    "        \n",
    "        # define a dropout layer\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        \n",
    "        # define the final, fully-connected output layer. We can use a \n",
    "        # PyTorch nn function here (or you could add the corresponding math\n",
    "        # below and assign an additional weight & bias at the top). \n",
    "        # We can see that we can create very custom models this way\n",
    "        self.fc = nn.Linear(self.hidden_sz, self.n_chars)\n",
    "        \n",
    "        # We have assigned the Parameters above, but we will need to also \n",
    "        # initialize them. Let's write a function for that and initialize\n",
    "        # our weights and bias. \n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        nn.init.xavier_uniform_(self.weight_ih)\n",
    "        nn.init.xavier_uniform_(self.weight_hh)\n",
    "        nn.init.zeros_(self.bias_hh)\n",
    "    \n",
    "    def forward(self, x, h_t):\n",
    "        \"\"\"Assumes x is of shape (batch, sequence, feature)\"\"\"\n",
    "        bs, seq_sz, _ = x.size()\n",
    "        hidden_seq = []\n",
    "        \n",
    "        # Given an input and an initial hidden state, calculate the next hidden\n",
    "        # state for each sequence element.\n",
    "        # We append all the hidden states to a list (similar to a batch size)\n",
    "        # so that we can concatenate them in the batch and feed them to our\n",
    "        # last linear layer all in parallel to avoid looping through the final\n",
    "        # output layer as there is no more dependence on other time steps. \n",
    "        for t in range(seq_sz):\n",
    "            x_t = x[:, t, :]\n",
    "            h_t = torch.tanh(x_t @ self.weight_ih + h_t @ self.weight_hh + self.bias_hh)\n",
    "            hidden_seq.append(h_t.unsqueeze(0))\n",
    "            \n",
    "        # Do the concatenation and reshaping for convenience\n",
    "        hidden_seq = torch.cat(hidden_seq, dim=0)\n",
    "        # reshape from shape (sequence, batch, feature) to (batch, sequence, feature)\n",
    "        hidden_seq = hidden_seq.transpose(0, 1).contiguous()\n",
    "        \n",
    "        # Stack up the RNN outputs using view so that we can process the last \n",
    "        # layer in parallel\n",
    "        r_output = hidden_seq.contiguous().view(-1, self.hidden_sz)\n",
    "        \n",
    "        # pass through a dropout layer\n",
    "        out = self.dropout(r_output)\n",
    "        \n",
    "        # Calculate fully connected layer output that yields our class vector\n",
    "        out = self.fc(out)\n",
    "        \n",
    "        return out, h_t\n",
    "    \n",
    "    def init_hidden(self, batch_size=1):\n",
    "        ''' Initializes hidden state '''\n",
    "        # This is a convenience function so that we can initialize a hidden\n",
    "        # state to zero when we start prediction on a sequence. Every further\n",
    "        # step will then depend on the previous hidden state. \n",
    "        \n",
    "        # Create two new tensors with sizes batch_size x n_hidden,\n",
    "        # initialized to zero for hidden the RNN's hidden state.\n",
    "        weight = next(self.parameters()).data\n",
    "        h_t = weight.new(batch_size, self.hidden_sz).zero_().to(device)\n",
    "        \n",
    "        return h_t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declaring the train method\n",
    "def train(model, data, device, optimizer, criterion, epochs=10, batch_size=10,\n",
    "          seq_length=50, clip=5):\n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # initialize first hidden states with zeros\n",
    "        h = model.init_hidden(batch_size)\n",
    "        \n",
    "        for x, y in get_batches(data, batch_size, seq_length):\n",
    "\n",
    "            # One-hot encode our data, make them torch tensors & cast to device\n",
    "            x = one_hot_encode(x, model.n_chars)\n",
    "            inputs, targets = torch.from_numpy(x), torch.from_numpy(y)\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "            # zero accumulated gradients\n",
    "            model.zero_grad()\n",
    "            \n",
    "            # get the output and hidden state from the model\n",
    "            output, h = model(inputs, h)\n",
    "            \n",
    "            # calculate the loss and perform backprop\n",
    "            # because we have flattened our batch and sequence in the model to \n",
    "            # be able to speed up the connection of the last fully-connected \n",
    "            # layer we now also need to view/flatten our target here\n",
    "            loss = criterion(output, targets.view(batch_size*seq_length).long())\n",
    "            loss.backward(retain_graph=True)\n",
    "            \n",
    "            # we use an additional trick of clipping gradients to avoid \n",
    "            # exploding gradients, which is a prominent problem in RNNs, just\n",
    "            # as the opposite problem of vanishing gradients.\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "            optimizer.step()\n",
    "            \n",
    "            \n",
    "        print(\"Epoch: {}/{}:\".format(epoch + 1, epochs),\n",
    "              \"Loss: {:.4f}:\".format(loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/100: Loss: 3.4647:\n",
      "Epoch: 2/100: Loss: 3.1898:\n",
      "Epoch: 3/100: Loss: 6.9307:\n",
      "Epoch: 4/100: Loss: 3.4230:\n",
      "Epoch: 5/100: Loss: 3.2569:\n",
      "Epoch: 6/100: Loss: 3.2210:\n",
      "Epoch: 7/100: Loss: 3.1619:\n",
      "Epoch: 8/100: Loss: 3.1369:\n",
      "Epoch: 9/100: Loss: 3.1115:\n",
      "Epoch: 10/100: Loss: 3.0961:\n",
      "Epoch: 11/100: Loss: 3.0699:\n",
      "Epoch: 12/100: Loss: 3.0454:\n",
      "Epoch: 13/100: Loss: 3.0142:\n",
      "Epoch: 14/100: Loss: 2.9752:\n",
      "Epoch: 15/100: Loss: 2.9368:\n",
      "Epoch: 16/100: Loss: 2.8977:\n",
      "Epoch: 17/100: Loss: 2.8545:\n",
      "Epoch: 18/100: Loss: 2.8478:\n",
      "Epoch: 19/100: Loss: 2.7960:\n",
      "Epoch: 20/100: Loss: 2.7517:\n",
      "Epoch: 21/100: Loss: 2.7314:\n",
      "Epoch: 22/100: Loss: 2.7080:\n",
      "Epoch: 23/100: Loss: 2.6567:\n",
      "Epoch: 24/100: Loss: 2.6560:\n",
      "Epoch: 25/100: Loss: 2.5973:\n",
      "Epoch: 26/100: Loss: 2.5711:\n",
      "Epoch: 27/100: Loss: 2.5369:\n",
      "Epoch: 28/100: Loss: 2.5350:\n",
      "Epoch: 29/100: Loss: 2.4932:\n",
      "Epoch: 30/100: Loss: 2.4671:\n",
      "Epoch: 31/100: Loss: 2.4558:\n",
      "Epoch: 32/100: Loss: 2.4390:\n",
      "Epoch: 33/100: Loss: 2.4167:\n",
      "Epoch: 34/100: Loss: 2.3915:\n",
      "Epoch: 35/100: Loss: 2.3746:\n",
      "Epoch: 36/100: Loss: 2.3654:\n",
      "Epoch: 37/100: Loss: 2.3429:\n",
      "Epoch: 38/100: Loss: 2.3300:\n",
      "Epoch: 39/100: Loss: 2.3192:\n",
      "Epoch: 40/100: Loss: 2.3080:\n",
      "Epoch: 41/100: Loss: 2.2942:\n",
      "Epoch: 42/100: Loss: 2.2874:\n",
      "Epoch: 43/100: Loss: 2.2698:\n",
      "Epoch: 44/100: Loss: 2.2785:\n",
      "Epoch: 45/100: Loss: 2.2553:\n",
      "Epoch: 46/100: Loss: 2.2507:\n",
      "Epoch: 47/100: Loss: 2.2327:\n",
      "Epoch: 48/100: Loss: 2.2341:\n",
      "Epoch: 49/100: Loss: 2.2204:\n",
      "Epoch: 50/100: Loss: 2.2761:\n",
      "Epoch: 51/100: Loss: 2.2821:\n",
      "Epoch: 52/100: Loss: 2.2491:\n",
      "Epoch: 53/100: Loss: 2.2135:\n",
      "Epoch: 54/100: Loss: 2.2039:\n",
      "Epoch: 55/100: Loss: 2.1878:\n",
      "Epoch: 56/100: Loss: 2.1845:\n",
      "Epoch: 57/100: Loss: 2.1774:\n",
      "Epoch: 58/100: Loss: 2.1702:\n",
      "Epoch: 59/100: Loss: 2.1629:\n",
      "Epoch: 60/100: Loss: 2.1562:\n",
      "Epoch: 61/100: Loss: 2.1561:\n",
      "Epoch: 62/100: Loss: 2.1569:\n",
      "Epoch: 63/100: Loss: 2.1424:\n",
      "Epoch: 64/100: Loss: 2.1436:\n",
      "Epoch: 65/100: Loss: 2.1324:\n",
      "Epoch: 66/100: Loss: 2.1316:\n",
      "Epoch: 67/100: Loss: 2.1162:\n",
      "Epoch: 68/100: Loss: 2.2060:\n",
      "Epoch: 69/100: Loss: 2.1463:\n",
      "Epoch: 70/100: Loss: 2.1318:\n",
      "Epoch: 71/100: Loss: 2.1188:\n",
      "Epoch: 72/100: Loss: 2.1094:\n",
      "Epoch: 73/100: Loss: 2.1001:\n",
      "Epoch: 74/100: Loss: 2.0933:\n",
      "Epoch: 75/100: Loss: 2.0834:\n",
      "Epoch: 76/100: Loss: 2.0815:\n",
      "Epoch: 77/100: Loss: 2.0768:\n",
      "Epoch: 78/100: Loss: 2.0768:\n",
      "Epoch: 79/100: Loss: 2.0674:\n",
      "Epoch: 80/100: Loss: 2.0691:\n",
      "Epoch: 81/100: Loss: 2.0610:\n",
      "Epoch: 82/100: Loss: 2.0545:\n",
      "Epoch: 83/100: Loss: 2.0477:\n",
      "Epoch: 84/100: Loss: 2.0563:\n",
      "Epoch: 85/100: Loss: 2.0444:\n",
      "Epoch: 86/100: Loss: 2.0411:\n",
      "Epoch: 87/100: Loss: 2.0384:\n",
      "Epoch: 88/100: Loss: 2.0295:\n",
      "Epoch: 89/100: Loss: 2.0201:\n",
      "Epoch: 90/100: Loss: 2.0201:\n",
      "Epoch: 91/100: Loss: 2.0134:\n",
      "Epoch: 92/100: Loss: 2.0095:\n",
      "Epoch: 93/100: Loss: 2.0219:\n",
      "Epoch: 94/100: Loss: 2.0089:\n",
      "Epoch: 95/100: Loss: 2.0120:\n",
      "Epoch: 96/100: Loss: 2.0038:\n",
      "Epoch: 97/100: Loss: 1.9990:\n",
      "Epoch: 98/100: Loss: 1.9868:\n",
      "Epoch: 99/100: Loss: 1.9776:\n",
      "Epoch: 100/100: Loss: 1.9689:\n"
     ]
    }
   ],
   "source": [
    "# Define the model\n",
    "n_hidden=512\n",
    "model = RNN(chars, device, n_hidden).to(device)\n",
    "\n",
    "# Hyperparameters\n",
    "batch_size = 128\n",
    "seq_length = 100\n",
    "epochs = 100 # start with 50 or similar if you are debugging \n",
    "# train much longer if you want good results\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# train the model\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "train(model, encoded, device, optimizer, criterion, epochs=epochs,\n",
    "      batch_size=batch_size, seq_length=seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, char, device, h=None, top_k=5):\n",
    "        ''' Given a character & hidden state, predict the next character.\n",
    "            Returns the predicted character and the hidden state.\n",
    "        '''\n",
    "        \n",
    "        # tensor inputs\n",
    "        x = np.array([[model.char2int[char]]])\n",
    "        x = one_hot_encode(x, model.n_chars)\n",
    "        inputs = torch.from_numpy(x).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # get the output of the model\n",
    "            out, h = model(inputs, h)\n",
    "\n",
    "            # get the character probabilities\n",
    "            # move to cpu for further processing with numpy etc. \n",
    "            p = F.softmax(out, dim=1).data.cpu()\n",
    "\n",
    "            # get the top characters with highest likelihood\n",
    "            p, top_ch = p.topk(top_k)\n",
    "            top_ch = top_ch.numpy().squeeze()\n",
    "\n",
    "            # select the likely next character with some element of randomness\n",
    "            # for more variability\n",
    "            p = p.numpy().squeeze()\n",
    "            char = np.random.choice(top_ch, p=p/p.sum())\n",
    "        \n",
    "        # return the encoded value of the predicted char and the hidden state\n",
    "        return model.int2char[char], h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(model, size, device, prime='A', top_k=None):\n",
    "    # method to generate new text based on a \"prime\"/initial sequence. \n",
    "    # Basically, the outer loop convenience function that calls the above\n",
    "    # defined predict method. \n",
    "    model.eval() # eval mode\n",
    "    \n",
    "    # Calculate model for the initial prime characters\n",
    "    chars = [ch for ch in prime]\n",
    "    with torch.no_grad():\n",
    "        # initialize hidden with 0 in the beginning. Set our batch size to 1 \n",
    "        # as we wish to generate one sequence only. \n",
    "        h = model.init_hidden(batch_size=1)\n",
    "        for ch in prime:\n",
    "            char, h = predict(model, ch, device, h=h, top_k=top_k)\n",
    "\n",
    "        # append the characters to the sequence\n",
    "        chars.append(char)\n",
    "\n",
    "        # Now pass in the previous/last character and get a new one\n",
    "        # Repeat this process for the desired length of the sequence to be \n",
    "        # generated\n",
    "        for ii in range(size):\n",
    "            char, h = predict(model, chars[-1], device, h=h, top_k=top_k)\n",
    "            chars.append(char)\n",
    "\n",
    "    return ''.join(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Asw fare thou mere,\n",
      "Then thou mist lfat, tere ingust wered frrat thing wald this tay the wall tin willd sathe for neas non the portered se fantis dost hos then se facr ant, bes beath st ald meses atenser, in the round this this peen bred woth,\n",
      "Ands ald wing that thee thou hos arthen tom hitg tont ferm stel ghoun see seen,\n",
      "Thet il his felthiss ow the thit, th wisl worl weet be tay sull,\n",
      "As the best love sor thal,\n",
      "I do geat y aldesus test beat why shis thin, ald thes with whit thes ithor sored, wath ton sans,\n",
      "Whin to mat ains besury'd hand with worch tr and.\n",
      "Or houn thes that the tore be that beast oun sen toul oving inded iras nout sheald and wred\n",
      "The sthat stoun whotrs if that tee thor with nost beat youg tenet, stor hat theree fees tore wall wore,\n",
      "Whan thou tort thas en whed,\n",
      "Whoch bete wort watl will th me teren ald be the singes buse fist wirls to me tind leveny\n",
      "Whet,\n",
      "Whechire bus speesse fot dene seerid,\n",
      "And sull seare thing teer my than to lim hich so fure thit te thie pingur stamy,\n"
     ]
    }
   ],
   "source": [
    "print(sample(model, 1000, device, prime='A', top_k=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlpvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
