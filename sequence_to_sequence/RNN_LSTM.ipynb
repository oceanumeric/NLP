{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN and Long Short-Term Memory (LSTM) recurrent neural network cells\n",
    "\n",
    "We will implement RNN and LSTM cells from scratch and use them to build a language model. We will then train the model on a dataset of Shakespeare's writing.\n",
    "\n",
    "Like it says, practice makes perfect. So, let's get started!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages that are not related to torch\n",
    "import os\n",
    "import math\n",
    "import time\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# torch import\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as tu_data\n",
    "from torchvision.datasets import FashionMNIST\n",
    "\n",
    "\n",
    "### --------- environment setup --------- ###\n",
    "# set up the data path\n",
    "DATA_PATH = \"../GPT-2/data\"\n",
    "\n",
    "# function for setting seed\n",
    "def set_seed(seed):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        \n",
    "# set up seed globally and deterministically\n",
    "set_seed(42)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# set up device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device:  cuda\n"
     ]
    }
   ],
   "source": [
    "print(\"Device: \", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the data\n",
    "with open(os.path.join(DATA_PATH, \"input.txt\"), \"r\") as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You\n"
     ]
    }
   ],
   "source": [
    "print(text[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n', ' ', '!', '$', '&', \"'\", ',', '-', '.', '3', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
      "The number of unique characters: 65\n",
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "print(chars)\n",
    "print(\"The number of unique characters: {}\".format(len(chars)))\n",
    "print(\"\".join(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'\\n': 0, ' ': 1, '!': 2, '$': 3, '&': 4, \"'\": 5, ',': 6, '-': 7, '.': 8, '3': 9, ':': 10, ';': 11, '?': 12, 'A': 13, 'B': 14, 'C': 15, 'D': 16, 'E': 17, 'F': 18, 'G': 19, 'H': 20, 'I': 21, 'J': 22, 'K': 23, 'L': 24, 'M': 25, 'N': 26, 'O': 27, 'P': 28, 'Q': 29, 'R': 30, 'S': 31, 'T': 32, 'U': 33, 'V': 34, 'W': 35, 'X': 36, 'Y': 37, 'Z': 38, 'a': 39, 'b': 40, 'c': 41, 'd': 42, 'e': 43, 'f': 44, 'g': 45, 'h': 46, 'i': 47, 'j': 48, 'k': 49, 'l': 50, 'm': 51, 'n': 52, 'o': 53, 'p': 54, 'q': 55, 'r': 56, 's': 57, 't': 58, 'u': 59, 'v': 60, 'w': 61, 'x': 62, 'y': 63, 'z': 64}\n",
      "{0: '\\n', 1: ' ', 2: '!', 3: '$', 4: '&', 5: \"'\", 6: ',', 7: '-', 8: '.', 9: '3', 10: ':', 11: ';', 12: '?', 13: 'A', 14: 'B', 15: 'C', 16: 'D', 17: 'E', 18: 'F', 19: 'G', 20: 'H', 21: 'I', 22: 'J', 23: 'K', 24: 'L', 25: 'M', 26: 'N', 27: 'O', 28: 'P', 29: 'Q', 30: 'R', 31: 'S', 32: 'T', 33: 'U', 34: 'V', 35: 'W', 36: 'X', 37: 'Y', 38: 'Z', 39: 'a', 40: 'b', 41: 'c', 42: 'd', 43: 'e', 44: 'f', 45: 'g', 46: 'h', 47: 'i', 48: 'j', 49: 'k', 50: 'l', 51: 'm', 52: 'n', 53: 'o', 54: 'p', 55: 'q', 56: 'r', 57: 's', 58: 't', 59: 'u', 60: 'v', 61: 'w', 62: 'x', 63: 'y', 64: 'z'}\n"
     ]
    }
   ],
   "source": [
    "# create index\n",
    "char2idx = {ch: i for i, ch in enumerate(chars)}\n",
    "idx2char = {i: ch for i, ch in enumerate(chars)}\n",
    "print(char2idx)\n",
    "print(idx2char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[46, 43, 50, 50, 53]\n",
      "hello\n"
     ]
    }
   ],
   "source": [
    "# encdoe and decode functions\n",
    "encode = lambda text: [char2idx[ch] for ch in text]\n",
    "decode = lambda tnsr: \"\".join([idx2char[i] for i in tnsr])\n",
    "\n",
    "# test the encode and decode functions\n",
    "print(encode(\"hello\"))\n",
    "print(decode(encode(\"hello\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data loader for each batch\n",
    "# return a generator\n",
    "def get_batch(encoded_text, seq_len, batch_size):\n",
    "    \"\"\"\n",
    "    encoded_text: the encoded text\n",
    "    seq_len: the length of the sequence\n",
    "    batch_size: the size of the batch\n",
    "    \"\"\"\n",
    "    # calculate the number of batches\n",
    "    n_batches = encoded_text.shape[0] // (seq_len * batch_size)\n",
    "    # reshape the encoded text\n",
    "    encoded_text = encoded_text[:n_batches * batch_size * seq_len]\n",
    "    encoded_text = encoded_text.reshape((batch_size, -1))\n",
    "    # loop through the encoded text\n",
    "    for i in range(0, encoded_text.shape[1], seq_len):\n",
    "        # get the input and the target\n",
    "        X = encoded_text[:, i:i+seq_len]\n",
    "        Y = torch.zeros_like(X)\n",
    "        # this part is tricky, we need to shift the input and the target\n",
    "        # by one character and avoid the loop (meaning the last character\n",
    "        # of the target should not be the first character of the input)\n",
    "        Y[:, :-1], Y[:, -1] = X[:, 1:], encoded_text[:, i+seq_len]\n",
    "        yield X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 10]) torch.Size([32, 10])\n",
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47], device='cuda:0')\n",
      "tensor([47, 56, 57, 58,  1, 15, 47, 58, 47, 64], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# test it\n",
    "encoded_text = torch.tensor(encode(text), dtype=torch.long, device=device)\n",
    "foo_x, foo_y = next(get_batch(encoded_text, 10, 32))\n",
    "print(foo_x.shape, foo_y.shape)\n",
    "print(foo_x[0])\n",
    "print(foo_y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 65])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.one_hot(foo_x[0], num_classes=len(chars)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for this kind of model, we will not split the data into train and test\n",
    "# as we are not doing classification, we are doing generation\n",
    "# so we will use all the data for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will wrint RNN again but this time we will use PyTorch's framework\n",
    "# it is important to do inheritance here from nn.Module\n",
    "class RNN(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, hidden_size, dropout=0.5, bias=True):\n",
    "        super().__init__()\n",
    "\n",
    "        # we will not use embedding layer here\n",
    "        # instead, we will use one-hot encoding\n",
    "        # so the input size will be the vocab size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.dropout = dropout\n",
    "\n",
    "        # initialize the weights by following the rule in the paper\n",
    "        # or you can use nn.init.xavier_uniform_ or kaime_uniform_\n",
    "        self.Wxh = nn.init.kaiming_uniform_(torch.empty(vocab_size, hidden_size, device=device))\n",
    "        self.Whh = nn.init.kaiming_uniform_(torch.empty(hidden_size, hidden_size, device=device))\n",
    "        self.bh = torch.zeros(hidden_size, device=device)\n",
    "\n",
    "        if bias:\n",
    "            self.bh = torch.zeros(hidden_size, device=device)\n",
    "            # no need to do by as we will use a linear layer\n",
    "        else:\n",
    "            self.bh = None\n",
    "\n",
    "\n",
    "        # define a dropout layer\n",
    "        self.dropout_layer = nn.Dropout(dropout)\n",
    "        # define a linear layer\n",
    "        self.linear_layer = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, x, h_t):\n",
    "        \"\"\"\n",
    "        x: the input, shape = (batch_size, seq_len, embedding_size)\n",
    "            if no embedding layer, shape = (batch_size, seq_len, vocab_size)\n",
    "        h_t: the hidden state at time t, shape = (batch_size, hidden_size)\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        hidden_sequences = []\n",
    "\n",
    "        # loop through the sequence\n",
    "        for t in range(seq_len):\n",
    "            # get the current input\n",
    "            x_t = x[:, t, :]\n",
    "            # calculate the hidden state\n",
    "            h_t = torch.tanh(x_t @ self.Wxh + h_t @ self.Whh + self.bh)\n",
    "            # append the hidden state, shape = (batch_size, hidden_size)\n",
    "            hidden_sequences.append(h_t) \n",
    "        \n",
    "        # stack the hidden states, shape = (seq_len, batch_size, hidden_size)\n",
    "        hidden_sequences = torch.stack(hidden_sequences)\n",
    "        # transpose the hidden states, shape = (batch_size, seq_len, hidden_size)\n",
    "        hidden_sequences = hidden_sequences.view(batch_size, seq_len, -1)\n",
    "\n",
    "        # stack the hidden states, shape = (batch_size * seq_len, hidden_size)\n",
    "        # we need to do this because we will use a linear layer\n",
    "        hidden_sequences = hidden_sequences.view(-1, self.hidden_size)\n",
    "        # hidden_sequences.shape = (batch_size * seq_len, hidden_size)\n",
    "        # apply dropout\n",
    "        hidden_sequences = self.dropout_layer(hidden_sequences)\n",
    "        # apply the linear layer\n",
    "        logits = self.linear_layer(hidden_sequences)\n",
    "        # logits.shape = (batch_size * seq_len, vocab_size)\n",
    "\n",
    "        # we need to return the logits and the last hidden state\n",
    "        return logits, h_t\n",
    "    \n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        \"\"\"\n",
    "        initialize the hidden state\n",
    "        \"\"\"\n",
    "        h0 = torch.zeros(batch_size, self.hidden_size)\n",
    "        # push to device\n",
    "        h0 = h0.to(device)\n",
    "        return h0\n",
    "    \n",
    "    def parameters(self):\n",
    "        \"\"\"\n",
    "        return all the parameters\n",
    "        \"\"\"\n",
    "        params = [self.Wxh, self.Whh]\n",
    "        if self.bh is not None:\n",
    "            params.append(self.bh)\n",
    "        # add the parameters of the linear layer and dropout layer\n",
    "        params += list(self.linear_layer.parameters())\n",
    "        params += list(self.dropout_layer.parameters())\n",
    "        return params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0, loss: 4.2684\n",
      "step: 2000, loss: 3.3287\n",
      "step: 4000, loss: 3.2865\n",
      "step: 6000, loss: 3.2795\n",
      "step: 8000, loss: 3.2803\n"
     ]
    }
   ],
   "source": [
    "# test the RNN\n",
    "max_steps = 10000\n",
    "vocab_size = len(chars)\n",
    "hidden_size = 512\n",
    "batch_size = 128\n",
    "seq_len = 100\n",
    "learning_rate = 1e-3\n",
    "\n",
    "# encode the text\n",
    "encoded_text = torch.tensor(encode(text), dtype=torch.long, device=device)\n",
    "\n",
    "# initialize the model\n",
    "rnn = RNN(vocab_size, hidden_size, dropout=0.5, bias=True)\n",
    "rrnn = rnn.to(device)\n",
    "\n",
    "# initialize the optimizer\n",
    "optimizer = torch.optim.Adam(rnn.parameters(), lr=learning_rate)\n",
    "\n",
    "# initialize the hidden state\n",
    "h_t = rnn.init_hidden(batch_size)\n",
    "\n",
    "# begin training\n",
    "for i in range(max_steps):\n",
    "    # get the input and the target\n",
    "    X, Y = next(get_batch(encoded_text, seq_len, batch_size))\n",
    "    # turn X, Y into one-hot encoding\n",
    "    X = F.one_hot(X, vocab_size).float()\n",
    "    Y = F.one_hot(Y, vocab_size).float()\n",
    "    # X.shape = (batch_size, seq_len, vocab_size)\n",
    "    # Y.shape = (batch_size, seq_len, vocab_size)\n",
    "    # put X, Y on the device\n",
    "    X = X.to(device)\n",
    "    Y = Y.to(device)\n",
    "    # get the logits and the last hidden state\n",
    "    logits, h_t = rnn(X, h_t)\n",
    "    # logits.shape = (batch_size * seq_len, vocab_size)\n",
    "    # h_t.shape = (batch_size, hidden_size)\n",
    "    # y.shape = (batch_size * seq_len, vocab_size)\n",
    "    # reshape Y\n",
    "    Y = Y.view(-1, vocab_size)\n",
    "    # calculate the loss\n",
    "    loss = F.cross_entropy(logits, Y)\n",
    "    # zero the gradients\n",
    "    optimizer.zero_grad()\n",
    "    # backprop\n",
    "    loss.backward()\n",
    "    # clip the gradients\n",
    "    torch.nn.utils.clip_grad_norm_(rnn.parameters(), 5)\n",
    "    # update the parameters\n",
    "    optimizer.step()\n",
    "\n",
    "    if i % 2000 == 0:\n",
    "        print(f\"step: {i}, loss: {loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, char, device, h=None, top_k=5):\n",
    "        ''' Given a character & hidden state, predict the next character.\n",
    "            Returns the predicted character and the hidden state.\n",
    "        '''\n",
    "        \n",
    "        # tensor inputs\n",
    "        x = np.array([[char2int[char]]])\n",
    "        x = F.one_hot(torch.from_numpy(x), len(model.chars)).float().to(device)\n",
    "        inputs = torch.from_numpy(x).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # get the output of the model\n",
    "            out, h = model(inputs, h)\n",
    "\n",
    "            # get the character probabilities\n",
    "            # move to cpu for further processing with numpy etc. \n",
    "            p = F.softmax(out, dim=1).data.cpu()\n",
    "\n",
    "            # get the top characters with highest likelihood\n",
    "            p, top_ch = p.topk(top_k)\n",
    "            top_ch = top_ch.numpy().squeeze()\n",
    "\n",
    "            # select the likely next character with some element of randomness\n",
    "            # for more variability\n",
    "            p = p.numpy().squeeze()\n",
    "            char = np.random.choice(top_ch, p=p/p.sum())\n",
    "        \n",
    "        # return the encoded value of the predicted char and the hidden state\n",
    "        return int2char[char], h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(model, size, device, prime='A', top_k=None):\n",
    "    # method to generate new text based on a \"prime\"/initial sequence. \n",
    "    # Basically, the outer loop convenience function that calls the above\n",
    "    # defined predict method. \n",
    "    model.eval() # eval mode\n",
    "    \n",
    "    # Calculate model for the initial prime characters\n",
    "    chars = [ch for ch in prime]\n",
    "    with torch.no_grad():\n",
    "        # initialize hidden with 0 in the beginning. Set our batch size to 1 \n",
    "        # as we wish to generate one sequence only. \n",
    "        h = model.init_hidden(batch_size=1)\n",
    "        for ch in prime:\n",
    "            char, h = predict(model, ch, device, h=h, top_k=top_k)\n",
    "\n",
    "        # append the characters to the sequence\n",
    "        chars.append(char)\n",
    "\n",
    "        # Now pass in the previous/last character and get a new one\n",
    "        # Repeat this process for the desired length of the sequence to be \n",
    "        # generated\n",
    "        for ii in range(size):\n",
    "            char, h = predict(model, chars[-1], device, h=h, top_k=top_k)\n",
    "            chars.append(char)\n",
    "\n",
    "    return ''.join(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlpvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
