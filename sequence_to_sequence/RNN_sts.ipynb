{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence to Sequence Learning with Neural Networks\n",
    "\n",
    "Sutskever, I., Vinyals, O., & Le, Q. V. (2014). Sequence to sequence learning with neural networks. Advances in neural information processing systems, 27.\n",
    "\n",
    "__Goal__: to translate a sentence from German to English."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import spacy\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "\n",
    "from collections import Counter, OrderedDict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as tu_data\n",
    "\n",
    "# text api \n",
    "import torchtext\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use GPU if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set random seeds for reproducibility\n",
    "SEED = 1234\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.use_deterministic_algorithms(True)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download the spacy models via command line:\n",
    "# python -m spacy download en\n",
    "# or in a jupyter notebook cell:\n",
    "# !python -m spacy download en\n",
    "# python -m spacy download de"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the spacy models for English and German\n",
    "spacy_de = spacy.load('de_core_news_sm')\n",
    "spacy_en = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello', 'world', '!']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# take a look at the function of the tokenizer\n",
    "[token.text for token in spacy_en.tokenizer('Hello world!')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the tokenizer function\n",
    "def tokenize_de(text):\n",
    "    \"\"\"\n",
    "    Tokenizes German text from a string into a list of strings \n",
    "    (tokens) and reverses it\n",
    "    authors of the paper found that reversing the order of the\n",
    "    source sentence improved performance\n",
    "    \"\"\"\n",
    "    return [tok.text for tok in spacy_de.tokenizer(text)][::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the tokenizer function\n",
    "def tokenize_en(text):\n",
    "    \"\"\"\n",
    "    Tokenizes English text from a string into a list of strings \n",
    "    (tokens)\n",
    "    \"\"\"\n",
    "    return [tok.text for tok in spacy_en.tokenizer(text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['!', 'Morgen', 'Guten']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize_de(\"Guten Morgen!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Good', 'morning', '!']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize_en(\"Good morning!\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch 2.0 Implementation\n",
    "\n",
    "I notice that APIs of Pytorch 2.0 are different from those of Pytorch 1.0. So I rewrite the code\n",
    "in Pytorch 2.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC_LANGUAGE = 'de'\n",
    "TGT_LANGUAGE = 'en'\n",
    "\n",
    "# Place-holders\n",
    "token_transform = {}\n",
    "vocab_transform = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_transform[SRC_LANGUAGE] = get_tokenizer('spacy', language='de_core_news_sm')\n",
    "token_transform[TGT_LANGUAGE] = get_tokenizer('spacy', language='en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Guten', 'Morgen', '!']"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_transform[SRC_LANGUAGE]('Guten Morgen!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['!', 'morning', 'Good']"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_transform[SRC_LANGUAGE]('Good morning!')[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the data from original source\n",
    "URL = {\n",
    "    \"train\": \"http://www.quest.dcs.shef.ac.uk/wmt16_files_mmt/training.tar.gz\",\n",
    "    \"valid\": \"http://www.quest.dcs.shef.ac.uk/wmt16_files_mmt/validation.tar.gz\",\n",
    "    \"test\": \"http://www.quest.dcs.shef.ac.uk/wmt16_files_mmt/mmt16_task1_test.tar.gz\",\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1.21M/1.21M [00:00<00:00, 2.45MB/s]\n",
      "100%|██████████| 46.3k/46.3k [00:00<00:00, 1.25MB/s]\n",
      "100%|██████████| 43.9k/43.9k [00:00<00:00, 864kB/s]\n"
     ]
    }
   ],
   "source": [
    "for (split, url) in URL.items():\n",
    "    torchtext.utils.download_from_url(url, root=\"./textdata\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the data\n",
    "import tarfile\n",
    "\n",
    "tar_file = ['training.tar.gz', 'validation.tar.gz', 'mmt16_task1_test.tar.gz']\n",
    "\n",
    "for file in tar_file:\n",
    "    with tarfile.open('./textdata/'+file, 'r:gz') as tar:\n",
    "        tar.extractall('./textdata')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mmt16_task1_test.tar.gz  test.en   train.en\t    val.de  validation.tar.gz\n",
      "test.de\t\t\t train.de  training.tar.gz  val.en\n"
     ]
    }
   ],
   "source": [
    "# take a look at the data\n",
    "!ls ./textdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read all files with .en or .de suffix\n",
    "# and save them in a list\n",
    "text_data_dict = {}\n",
    "\n",
    "for file in os.listdir('./textdata'):\n",
    "    if file.endswith('.en') or file.endswith('.de'):\n",
    "        with open('./textdata/'+file, 'r') as f:\n",
    "            text_data_dict[file] = f.read().splitlines()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['val.en', 'train.de', 'test.de', 'train.en', 'val.de', 'test.en'])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# take a look at the data\n",
    "text_data_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------val.en:\n",
      " A group of men are loading cotton onto a truck\n",
      "\n",
      "------------train.de:\n",
      " Zwei junge weiße Männer sind im Freien in der Nähe vieler Büsche.\n",
      "\n",
      "------------test.de:\n",
      " Ein Mann mit einem orangefarbenen Hut, der etwas anstarrt.\n",
      "\n",
      "------------train.en:\n",
      " Two young, White males are outside near many bushes.\n",
      "\n",
      "------------val.de:\n",
      " Eine Gruppe von Männern lädt Baumwolle auf einen Lastwagen\n",
      "\n",
      "------------test.en:\n",
      " A man in an orange hat starring at something.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print out some examples\n",
    "for key in text_data_dict.keys():\n",
    "     print(f\"------------{key}:\\n {text_data_dict[key][0]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 29001\n",
      "Number of validation examples: 29001\n",
      "Number of test examples: 1000\n",
      "Number of test examples: 1000\n"
     ]
    }
   ],
   "source": [
    "# print out the number of examples\n",
    "print(f\"Number of training examples: {len(text_data_dict['train.en'])}\")\n",
    "print(f\"Number of validation examples: {len(text_data_dict['train.en'])}\")\n",
    "print(f\"Number of test examples: {len(text_data_dict['test.en'])}\")\n",
    "print(f\"Number of test examples: {len(text_data_dict['test.de'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing train.de and train.en ...\n",
      "Processing val.de and val.en ...\n",
      "Processing test.de and test.en ...\n"
     ]
    }
   ],
   "source": [
    "# build up the train and valid data\n",
    "train_data = {'split': 'train', 'src': [], 'trg': []}\n",
    "valid_data = {'split': 'val', 'src': [], 'trg': []}\n",
    "test_data = {'split': 'test', 'src': [], 'trg': []}\n",
    "\n",
    "# define a function to build up the data\n",
    "def tokenize_data():\n",
    "    splits = [train_data, valid_data, test_data]\n",
    "\n",
    "    for sdata in splits:\n",
    "        key_temp_src = sdata['split'] + '.de'\n",
    "        key_temp_trg = sdata['split'] + '.en'\n",
    "        print(f\"Processing {key_temp_src} and {key_temp_trg} ...\")\n",
    "        if sdata['split'] in ['train', 'val']:\n",
    "            for i in range(len(text_data_dict[key_temp_src])-1):\n",
    "                setenct_src_temp = text_data_dict[key_temp_src][i]\n",
    "                setenct_trg_temp = text_data_dict[key_temp_trg][i]\n",
    "\n",
    "                # tokenize the source and target sentences\n",
    "                setenct_src_temp = token_transform[SRC_LANGUAGE](setenct_src_temp)[::-1]\n",
    "                setenct_trg_temp = token_transform[TGT_LANGUAGE](setenct_trg_temp)\n",
    "\n",
    "                # lower case the src and trg sentences\n",
    "                setenct_src_temp = [t.lower() for t in setenct_src_temp]\n",
    "                setenct_trg_temp = [t.lower() for t in setenct_trg_temp]\n",
    "\n",
    "                # if the first or last token is '.', remove it\n",
    "                # if setenct_src_temp[0] == '.':\n",
    "                #     setenct_src_temp = setenct_src_temp[1:]\n",
    "                # if setenct_src_temp[-1] == '.':\n",
    "                #     setenct_src_temp = setenct_src_temp[:-1]\n",
    "                \n",
    "                # if setenct_trg_temp[0] == '.':\n",
    "                #     setenct_trg_temp = setenct_trg_temp[1:]\n",
    "                # if setenct_trg_temp[-1] == '.':\n",
    "                #     setenct_trg_temp = setenct_trg_temp[:-1]\n",
    "\n",
    "                # append the tokenized source and target sentences\n",
    "                sdata['src'].append(setenct_src_temp)\n",
    "                sdata['trg'].append(setenct_trg_temp)\n",
    "        else:\n",
    "            for i in range(len(text_data_dict[key_temp_src])):\n",
    "                setenct_src_temp = text_data_dict[key_temp_src][i]\n",
    "                setenct_trg_temp = text_data_dict[key_temp_trg][i]\n",
    "\n",
    "                # tokenize the source and target sentences\n",
    "                setenct_src_temp = token_transform[SRC_LANGUAGE](setenct_src_temp)[::-1]\n",
    "                setenct_trg_temp = token_transform[TGT_LANGUAGE](setenct_trg_temp)\n",
    "\n",
    "                # lower case the src and trg sentences\n",
    "                setenct_src_temp = [t.lower() for t in setenct_src_temp]\n",
    "                setenct_trg_temp = [t.lower() for t in setenct_trg_temp]\n",
    "\n",
    "                # if setenct_src_temp[0] == '.':\n",
    "                #     setenct_src_temp = setenct_src_temp[1:]\n",
    "                # if setenct_src_temp[-1] == '.':\n",
    "                #     setenct_src_temp = setenct_src_temp[:-1]\n",
    "                \n",
    "                # if setenct_trg_temp[0] == '.':\n",
    "                #     setenct_trg_temp = setenct_trg_temp[1:]\n",
    "                # if setenct_trg_temp[-1] == '.':\n",
    "                #     setenct_trg_temp = setenct_trg_temp[:-1]\n",
    "\n",
    "                # append the tokenized source and target sentences\n",
    "                sdata['src'].append(setenct_src_temp)\n",
    "                sdata['trg'].append(setenct_trg_temp)\n",
    "\n",
    "tokenize_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.', 'büsche', 'vieler', 'nähe', 'der', 'in', 'freien', 'im', 'sind', 'männer', 'weiße', 'junge', 'zwei'] \n",
      " ['two', 'young', ',', 'white', 'males', 'are', 'outside', 'near', 'many', 'bushes', '.']\n"
     ]
    }
   ],
   "source": [
    "# take a look at the train_data\n",
    "print(train_data['src'][0], '\\n', train_data['trg'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['anstarrt', 'etwas', 'der', ',', 'hut', 'orangefarbenen', 'einem', 'mit', 'mann', 'ein'] \n",
      " ['a', 'man', 'in', 'an', 'orange', 'hat', 'starring', 'at', 'something']\n"
     ]
    }
   ],
   "source": [
    "# print out test data example\n",
    "print(test_data['src'][0], '\\n', test_data['trg'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sofa', 'einem', 'auf', 'raum', 'grünen', 'einem', 'in', 'schläft', 'mann', 'ein'] \n",
      " ['a', 'man', 'sleeping', 'in', 'a', 'green', 'room', 'on', 'a', 'couch']\n"
     ]
    }
   ],
   "source": [
    "# print out the valid data example\n",
    "print(valid_data['src'][1], '\\n', valid_data['trg'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples for src: 29000\n",
      "Number of training examples for trg: 29000\n"
     ]
    }
   ],
   "source": [
    "# check train_data\n",
    "print(f\"Number of training examples for src: {len(train_data['src'])}\")\n",
    "print(f\"Number of training examples for trg: {len(train_data['trg'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now add the BOS and EOS tokens to the src and trg sentences\n",
    "BOS_WORD = '<s>'\n",
    "EOS_WORD = '</s>'\n",
    "\n",
    "# define a function to add the BOS and EOS tokens\n",
    "def add_bos_eos(data):\n",
    "    for i in range(len(data['src'])):\n",
    "        data['trg'][i] = [BOS_WORD] + data['trg'][i] + [EOS_WORD]\n",
    "        # since we reverse the source sentence\n",
    "        # we need to add the EOS token at the beginning\n",
    "        data['src'][i] = [EOS_WORD] + data['src'][i] + [BOS_WORD]\n",
    "\n",
    "    return data\n",
    "\n",
    "train_data = add_bos_eos(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples for src: 29000\n",
      "['</s>', '.', 'büsche', 'vieler', 'nähe', 'der', 'in', 'freien', 'im', 'sind', 'männer', 'weiße', 'junge', 'zwei', '<s>'] \n",
      " ['<s>', 'two', 'young', ',', 'white', 'males', 'are', 'outside', 'near', 'many', 'bushes', '.', '</s>']\n"
     ]
    }
   ],
   "source": [
    "# check train_data\n",
    "print(f\"Number of training examples for src: {len(train_data['src'])}\")\n",
    "# print out the first example\n",
    "print(train_data['src'][0], '\\n', train_data['trg'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we will build up the vocabulary\n",
    "# first we need to define the special tokens\n",
    "UNK_WORD = \"<unk>\"\n",
    "\n",
    "# define the special tokens\n",
    "SRC_SPECIALS = [UNK_WORD, BOS_WORD, EOS_WORD]\n",
    "\n",
    "# vocabulareis only shwon one time are treated as UNK_WORD\n",
    "SRC_MIN_FREQ = 2\n",
    "TRG_MIN_FREQ = 2\n",
    "\n",
    "# build up the vocabularies\n",
    "def build_vocab(data):\n",
    "    # build up the vocabularies\n",
    "    src_list = []\n",
    "    for src in data['src']:\n",
    "        src_list += src\n",
    "    trg_list = []\n",
    "    for trg in data['trg']:\n",
    "        trg_list += trg\n",
    "    # build up the counter  \n",
    "    src_counter = Counter(src_list)\n",
    "    trg_counter = Counter(trg_list)\n",
    "    # sort the counter\n",
    "    src_counter = sorted(src_counter.items(), key=lambda x: x[1], reverse=True)\n",
    "    trg_counter = sorted(trg_counter.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # create an ordered dictionary from the counter\n",
    "    src_dict = OrderedDict(src_counter)\n",
    "    trg_dict = OrderedDict(trg_counter)\n",
    "\n",
    "    # build up the vocabularies\n",
    "    src_vocab = vocab(src_dict, specials=SRC_SPECIALS, min_freq=SRC_MIN_FREQ)\n",
    "    trg_vocab = vocab(trg_dict, specials=SRC_SPECIALS, min_freq=TRG_MIN_FREQ)\n",
    "\n",
    "    return src_vocab, trg_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_src_vocab, train_trg_vocab = build_vocab(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique tokens in source (de) vocabulary: 7852\n",
      "Number of unique tokens in target (en) vocabulary: 5892\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of unique tokens in source (de) vocabulary: {len(train_src_vocab)}\")\n",
    "print(f\"Number of unique tokens in target (en) vocabulary: {len(train_trg_vocab)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torchtext.vocab.vocab.Vocab"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_src_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 <unk>\n",
      "1 <s>\n",
      "2 </s>\n",
      "3 a\n",
      "4 .\n",
      "5 in\n",
      "6 the\n",
      "7 on\n",
      "8 man\n",
      "9 is\n",
      "10 and\n"
     ]
    }
   ],
   "source": [
    "# set up the default index for the UNK_WORD\n",
    "for idx, token in enumerate(train_trg_vocab.get_itos()):\n",
    "    if idx in [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]:\n",
    "        print(idx, token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 <unk>\n",
      "1 <s>\n",
      "2 </s>\n",
      "3 .\n",
      "4 ein\n",
      "5 einem\n",
      "6 in\n",
      "7 eine\n",
      "8 ,\n",
      "9 und\n",
      "10 mit\n"
     ]
    }
   ],
   "source": [
    "for idx, token in enumerate(train_src_vocab.get_itos()):\n",
    "    if idx in [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]:\n",
    "        print(idx, token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up the default index for the UNK_WORD\n",
    "# this step is very important\n",
    "train_trg_vocab.set_default_index(train_trg_vocab['<unk>'])\n",
    "train_src_vocab.set_default_index(train_src_vocab['<unk>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with vocabularies, we can now convert the tokens into indices\n",
    "def data_process(data, src_vocab, trg_vocab):\n",
    "    # convert tokens into indices\n",
    "    for i in range(len(data['src'])):\n",
    "        data['src'][i] = src_vocab.forward(data['src'][i])\n",
    "        data['trg'][i] = trg_vocab.forward(data['trg'][i])\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "train_data = data_process(train_data, train_src_vocab, train_trg_vocab)\n",
    "valid_data = data_process(valid_data, train_src_vocab, train_trg_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 3, 3098, 5373, 109, 14, 6, 87, 19, 83, 29, 252, 25, 17, 1]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data['src'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 15, 23, 14, 24, 773, 16, 56, 79, 201, 1304, 4, 2]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data['trg'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 <s>\n",
      "2 </s>\n",
      "4 .\n",
      "14 ,\n",
      "15 two\n",
      "16 are\n",
      "23 young\n",
      "24 white\n",
      "56 outside\n",
      "79 near\n",
      "201 many\n",
      "773 males\n",
      "1304 bushes\n"
     ]
    }
   ],
   "source": [
    "for idx, token in enumerate(train_trg_vocab.get_itos()):\n",
    "    if idx in train_data['trg'][0]:\n",
    "        print(idx, token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 <s>\n",
      "2 </s>\n",
      "3 .\n",
      "6 in\n",
      "14 der\n",
      "17 zwei\n",
      "19 im\n",
      "25 junge\n",
      "29 männer\n",
      "83 sind\n",
      "87 freien\n",
      "109 nähe\n",
      "252 weiße\n",
      "3098 büsche\n",
      "5373 vieler\n"
     ]
    }
   ],
   "source": [
    "for idx, token in enumerate(train_src_vocab.get_itos()):\n",
    "    if idx in train_data['src'][0]:\n",
    "        print(idx, token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build up the model\n",
    "class Encoder(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, emb_dim, hidden_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "\n",
    "        # define the embedding layer\n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
    "\n",
    "        # define the LSTM layer\n",
    "        self.rnn = nn.LSTM(emb_dim, hidden_dim, n_layers, dropout=dropout)\n",
    "\n",
    "        # define the dropout layer\n",
    "        self.dropout = nn.Dropout(dropout)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
