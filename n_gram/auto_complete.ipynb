{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language Models Case: Auto Complete \n",
    "\n",
    "A language model assigns the probability to a sequence of words (quantitfying text). Later, the vector of probability\n",
    "will be used for other tasks like machine translation or speech recognition. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math \n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from collections import defaultdict\n",
    "from IPython.display import display\n",
    "nltk.data.path.append('.')\n",
    "random.seed(87)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "3335477\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"How are you? Btw thanks for the RT. You gonna be in DC anytime soon? Love to see you. Been way, way too long.\\nWhen you meet someone special... you'll know. Your heart will beat more rapidly and you'll smile for no reason.\\nthey've decided its more fun if I don't.\\nSo Tired D; Played Lazer Tag & Ran A \""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\"ust had one a few weeks back....hopefully we will be back soon! wish you the best yo\\nColombia is with an 'o'...“: We now ship to 4 countries in South America (fist pump). Please welcome Columbia to the Stunner Family”\\n#GutsiestMovesYouCanMake Giving a cat a bath.\\nCoffee after 5 was a TERRIBLE idea.\\n\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with open('en_US.twitter.txt', 'r') as f:\n",
    "    data = f.read()\n",
    "print(type(data))\n",
    "print(len(data))\n",
    "display(data[:300])\n",
    "display(data[-300:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-process the text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data by \"\\n\"\n",
    "def split_to_sentences(text):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        text: a series of strings \n",
    "    Output:\n",
    "        a list of sentences\n",
    "    \"\"\"\n",
    "    sentences_list = text.split(\"\\n\")\n",
    "    sentences_list = [s.strip() for s in sentences_list]  # strip out the space\n",
    "    sentences_list = [s for s in sentences_list if len(s) > 0]  # drop none\n",
    "    return sentences_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I have a pen.', 'I have an apple.', 'Ah', 'Apple pen.']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test \n",
    "x = \"\"\"\n",
    "I have a pen.\\n I have an apple. \\nAh \\n Apple pen.\\n\n",
    "\"\"\"\n",
    "split_to_sentences(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_sentences(text_list):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        test_list: like ['I have a pen', 'apple loves']\n",
    "    Output:\n",
    "        a list of token (words)\n",
    "    \"\"\"\n",
    "    token_list = []\n",
    "    for text in text_list:\n",
    "        text = text.lower()\n",
    "        tokenized = nltk.word_tokenize(text)\n",
    "        token_list.append(tokenized)\n",
    "        \n",
    "    return token_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['sky', 'is', 'blue', '.'],\n",
       " ['leaves', 'are', 'green', '.'],\n",
       " ['roses', 'are', 'red', '.']]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test your code\n",
    "sentences = [\"Sky is blue.\", \"Leaves are green.\", \"Roses are red.\"]\n",
    "tokenize_sentences(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_token(text):\n",
    "    sentences = split_to_sentences(text)\n",
    "    token = tokenize_sentences(sentences)\n",
    "    \n",
    "    return token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_split(data, train_percent, validation_percent):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        data: list of sentence\n",
    "        train_percent: e.g. 60%\n",
    "        validation_percent: e.g. 30%\n",
    "    Note: train_percent + validation_percent <= 100 \n",
    "    Output:\n",
    "        train_data: list of sentences \n",
    "        validation_data\n",
    "        test_data\n",
    "    \"\"\"\n",
    "    random.seed(87)\n",
    "    random.shuffle(data)\n",
    "    train_size = int(len(data) * train_percent /100)\n",
    "    train_data = data[:train_size]\n",
    "    \n",
    "    validation_size = int(len(data) * validation_percent / 100)\n",
    "    if train_percent + validation_percent == 100:\n",
    "        validatation_data = data[train_size:]\n",
    "    else:\n",
    "        validatation_data = data[train_size:train_size+validation_size]\n",
    "    \n",
    "    test_data = data[train_size+validation_size:]\n",
    "    \n",
    "    return train_data, validatation_data, test_data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_data = text_to_token(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data, validation_data, test_data = sample_split(tokenized_data, 80, 20)\n",
    "random.seed(87)\n",
    "random.shuffle(tokenized_data)\n",
    "train_size = int(len(tokenized_data) * 0.8)\n",
    "train_data = tokenized_data[0:train_size]\n",
    "test_data = tokenized_data[train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38368 9593\n",
      "['i', 'personally', 'would', 'like', 'as', 'our', 'official', 'glove', 'of', 'the', 'team', 'local', 'company', 'and', 'quality', 'production'] ['that', 'picture', 'i', 'just', 'seen', 'whoa', 'dere', '!', '!', '>', '>', '>', '>', '>', '>', '>']\n"
     ]
    }
   ],
   "source": [
    "print(len(train_data), len(test_data))\n",
    "print(train_data[0], test_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate words frequency\n",
    "def count_words(token_list):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        token list (WARNING: nexted list)\n",
    "    Output:\n",
    "        dict of words frequency \n",
    "    \"\"\"\n",
    "    words_freq = {}\n",
    "    \n",
    "    for token in token_list:\n",
    "        for word in token:\n",
    "            words_freq[word] = words_freq.get(word, 0) + 1\n",
    "            \n",
    "    return words_freq\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sky': 1,\n",
       " 'is': 1,\n",
       " 'blue': 1,\n",
       " '.': 3,\n",
       " 'leaves': 1,\n",
       " 'are': 2,\n",
       " 'green': 1,\n",
       " 'roses': 1,\n",
       " 'red': 1}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_token = [['sky', 'is', 'blue', '.'],\n",
    "                       ['leaves', 'are', 'green', '.'],\n",
    "                       ['roses', 'are', 'red', '.']]\n",
    "count_words(test_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build up a bag of words with frequency > n\n",
    "def hig_frequency_words(token_list, n):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        token_list: nested list of tokens\n",
    "    Output:\n",
    "        n: count threshold \n",
    "    \"\"\"\n",
    "    vocabulary = []\n",
    "    word_frequency = count_words(token_list)\n",
    "    for word, count in word_frequency.items():\n",
    "        if count >= n:\n",
    "            vocabulary.append(word)\n",
    "            \n",
    "    return vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.', 'are']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hig_frequency_words(test_token, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace out of vocabulary\n",
    "def replace_oov(token_list, vocabulary, unknow_token = \"<unk>\"):\n",
    "    vocabulary = set(vocabulary)  # set it as a set (union)\n",
    "    replaced_token_list = []\n",
    "    for token in token_list:\n",
    "        replaced_token = []\n",
    "        for word in token:\n",
    "            if word in vocabulary:\n",
    "                replaced_token.append(word)\n",
    "            else:\n",
    "                replaced_token.append(unknow_token)\n",
    "        replaced_token_list.append(replaced_token)\n",
    "    \n",
    "    return replaced_token_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['sky', '<unk>', '<unk>', '<unk>'],\n",
       " ['<unk>', '<unk>', 'green', '<unk>'],\n",
       " ['<unk>', '<unk>', '<unk>', '<unk>']]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary = ['sky', 'green']\n",
    "replace_oov(test_token, vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(train_data, test_data, count_threshold):\n",
    "    \"\"\"\n",
    "    preprocess data:\n",
    "        - find tokens that appear at least N times\n",
    "        - replace tokens that appear less than N times by \"<unk>\"\n",
    "    Input:\n",
    "        train_data\n",
    "        test_data\n",
    "        count_threshold\n",
    "    Output:\n",
    "        Tuple of:\n",
    "            trainning data with low frequency words replaced by \"<unk>\"\n",
    "            test data .......\n",
    "            vocabulary of words \n",
    "    \"\"\"\n",
    "    vocabulary = hig_frequency_words(train_data, n=count_threshold)\n",
    "    train_data_replaced = replace_oov(train_data, vocabulary)\n",
    "    test_data_replaced = replace_oov(test_data, vocabulary)\n",
    "    \n",
    "    return train_data_replaced, test_data_replaced, vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tmp_train_repl\n",
      "[['sky', 'is', 'blue', '.'], ['leaves', 'are', 'green']]\n",
      "\n",
      "tmp_test_repl\n",
      "[['<unk>', 'are', '<unk>', '.']]\n",
      "\n",
      "tmp_vocab\n",
      "['sky', 'is', 'blue', '.', 'leaves', 'are', 'green']\n"
     ]
    }
   ],
   "source": [
    "tmp_train = [['sky', 'is', 'blue', '.'],\n",
    "     ['leaves', 'are', 'green']]\n",
    "tmp_test = [['roses', 'are', 'red', '.']]\n",
    "\n",
    "tmp_train_repl, tmp_test_repl, tmp_vocab = preprocess_text(tmp_train, \n",
    "                                                           tmp_test, \n",
    "                                                           count_threshold = 1)\n",
    "\n",
    "print(\"tmp_train_repl\")\n",
    "print(tmp_train_repl)\n",
    "print()\n",
    "print(\"tmp_test_repl\")\n",
    "print(tmp_test_repl)\n",
    "print()\n",
    "print(\"tmp_vocab\")\n",
    "print(tmp_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing the train and test data\n",
    "minimum_freq = 2\n",
    "train_data_processed, test_data_processed, vocabulary = preprocess_text(train_data,\n",
    "                                                                        test_data, \n",
    "                                                                        minimum_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'personally', 'would', 'like', 'as', 'our', 'official', 'glove', 'of', 'the', 'team', 'local', 'company', 'and', 'quality', 'production'] ['that', 'picture', 'i', 'just', 'seen', 'whoa', 'dere', '!', '!', '>', '>', '>', '>', '>', '>', '>'] ['i', 'personally', 'would', 'like', 'as', 'our', 'official', 'glove', 'of', 'the'] 38368 9593 14823\n"
     ]
    }
   ],
   "source": [
    "print(train_data_processed[0], test_data_processed[0], vocabulary[0:10], \n",
    "      len(train_data_processed),\n",
    "      len(test_data_processed),\n",
    "      len(vocabulary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Develop a N-gram model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_n_grams(data, n, start_token='<s>', end_token='<e>'):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        data: list of lists of words\n",
    "        n: number of words in a sequence\n",
    "    Output:\n",
    "        a dict that maps a tuple of n-words to its frequency\n",
    "    \"\"\"\n",
    "    n_grams_dict = {}\n",
    "    for setence in data:\n",
    "        setence = [start_token] * n + setence + ['<e>']\n",
    "        setence = [tuple(setence[i:i+n]) for i in range(len(setence)-n+1)]\n",
    "        for i in range(len(setence)):\n",
    "            n_gram = setence[i]\n",
    "            n_grams_dict[n_gram] = n_grams_dict.get(n_gram, 0) + 1\n",
    "            \n",
    "    return n_grams_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uni-gram:\n",
      "{('<s>',): 2, ('i',): 1, ('like',): 2, ('a',): 2, ('cat',): 2, ('<e>',): 2, ('this',): 1, ('dog',): 1, ('is',): 1}\n",
      "Bi-gram:\n",
      "{('<s>', '<s>'): 2, ('<s>', 'i'): 1, ('i', 'like'): 1, ('like', 'a'): 2, ('a', 'cat'): 2, ('cat', '<e>'): 2, ('<s>', 'this'): 1, ('this', 'dog'): 1, ('dog', 'is'): 1, ('is', 'like'): 1}\n"
     ]
    }
   ],
   "source": [
    "sentences = [['i', 'like', 'a', 'cat'],\n",
    "             ['this', 'dog', 'is', 'like', 'a', 'cat']]\n",
    "print(\"Uni-gram:\")\n",
    "print(count_n_grams(sentences, 1))\n",
    "print(\"Bi-gram:\")\n",
    "print(count_n_grams(sentences, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_probability(word, n_gram_list, n_gram_dict, n_plus1_gram_dict, vocabulary_size, k = 1.0):\n",
    "    \"\"\"\n",
    "    Estmate the probabilities of a next word using the n-gram counts with k-smoothing\n",
    "    Input:\n",
    "        word: next word\n",
    "        n_gram_list\n",
    "        n_gram_dict: key + count\n",
    "        vocabulary_list\n",
    "    Output:\n",
    "        a probability\n",
    "    \"\"\"\n",
    "    previous_n_gram = tuple(n_gram_list)\n",
    "    previous_n_gram_count = n_gram_dict.get(previous_n_gram, 0)\n",
    "    denominator = previous_n_gram_count + k * vocabulary_size\n",
    "    \n",
    "    n_plus1_gram = previous_n_gram + (word, )  # do not use tuple(word) = ('w', 'o')\n",
    "    \n",
    "    numerator = n_plus1_gram_dict.get(n_plus1_gram, 0) + k\n",
    "    \n",
    "    probability = numerator / denominator\n",
    "    \n",
    "    return probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The estimated probability of word 'cat' given the previous n-gram 'a' is: 0.3333\n"
     ]
    }
   ],
   "source": [
    "sentences = [['i', 'like', 'a', 'cat'],\n",
    "             ['this', 'dog', 'is', 'like', 'a', 'cat']]\n",
    "unique_words = list(set(sentences[0] + sentences[1]))\n",
    "\n",
    "unigram_counts = count_n_grams(sentences, 1)\n",
    "bigram_counts = count_n_grams(sentences, 2)\n",
    "tmp_prob = estimate_probability(\"cat\", \"a\", unigram_counts, bigram_counts, len(unique_words), k=1)\n",
    "\n",
    "print(f\"The estimated probability of word 'cat' given the previous n-gram 'a' is: {tmp_prob:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_probabilities(n_gram, n_gram_dict, n_plus_gram_dict, vocabulary, k = 1.0):\n",
    "    previous_n_gram = tuple(n_gram)\n",
    "    vocabulary = vocabulary + [\"<e>\", \"<unk>\"]\n",
    "    vocabulary_size = len(vocabulary)\n",
    "    \n",
    "    probabilities = {}\n",
    "    for word in vocabulary:\n",
    "        prob = estimate_probability(word, previous_n_gram,\n",
    "                                    n_gram_dict,\n",
    "                                    n_plus_gram_dict,\n",
    "                                    vocabulary_size, k=k)\n",
    "        probabilities[word] = prob\n",
    "    \n",
    "    return probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'i': 0.09090909090909091,\n",
       " 'a': 0.09090909090909091,\n",
       " 'is': 0.09090909090909091,\n",
       " 'like': 0.09090909090909091,\n",
       " 'dog': 0.09090909090909091,\n",
       " 'this': 0.09090909090909091,\n",
       " 'cat': 0.2727272727272727,\n",
       " '<e>': 0.09090909090909091,\n",
       " '<unk>': 0.09090909090909091}"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test your code\n",
    "sentences = [['i', 'like', 'a', 'cat'],\n",
    "             ['this', 'dog', 'is', 'like', 'a', 'cat']]\n",
    "unique_words = list(set(sentences[0] + sentences[1]))\n",
    "unigram_counts = count_n_grams(sentences, 1)\n",
    "bigram_counts = count_n_grams(sentences, 2)\n",
    "estimate_probabilities(\"a\", unigram_counts, bigram_counts, unique_words, k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'i': 0.18181818181818182,\n",
       " 'a': 0.09090909090909091,\n",
       " 'is': 0.09090909090909091,\n",
       " 'like': 0.09090909090909091,\n",
       " 'dog': 0.09090909090909091,\n",
       " 'this': 0.18181818181818182,\n",
       " 'cat': 0.09090909090909091,\n",
       " '<e>': 0.09090909090909091,\n",
       " '<unk>': 0.09090909090909091}"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trigram_counts = count_n_grams(sentences, 3)\n",
    "estimate_probabilities([\"<s>\", \"<s>\"], bigram_counts, trigram_counts, unique_words, k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_count_matrix(n_plus1_gram_dict, vocabulary):\n",
    "    vocabulary = vocabulary + [\"<e>\", \"<unk>\"]\n",
    "    n_grams = []\n",
    "    for n_plus1_gram in n_plus1_gram_dict.keys():\n",
    "        n_gram = n_plus1_gram[0:-1]\n",
    "        n_grams.append(n_gram)\n",
    "    n_grams = list(set(n_grams))\n",
    "    \n",
    "    row_index = {n_gram: i for i, n_gram in enumerate(n_grams)}\n",
    "    col_index = {word:j for j, word in enumerate(vocabulary)}\n",
    "    nrow = len(n_grams)\n",
    "    ncol = len(vocabulary)\n",
    "    count_matrix = np.zeros((nrow, ncol))\n",
    "    for n_plus1_gram, count in n_plus1_gram_dict.items():\n",
    "        n_gram = n_plus1_gram[0:-1]\n",
    "        word = n_plus1_gram[-1]\n",
    "        if word not in vocabulary:\n",
    "            continue\n",
    "        i = row_index[n_gram]\n",
    "        j = col_index[word]\n",
    "        count_matrix[i, j] = count\n",
    "        \n",
    "    count_matrix = pd.DataFrame(count_matrix, index=n_grams, columns=vocabulary)\n",
    "    \n",
    "    return count_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bigram counts\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>i</th>\n",
       "      <th>a</th>\n",
       "      <th>is</th>\n",
       "      <th>like</th>\n",
       "      <th>dog</th>\n",
       "      <th>this</th>\n",
       "      <th>cat</th>\n",
       "      <th>&lt;e&gt;</th>\n",
       "      <th>&lt;unk&gt;</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>(like,)</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(cat,)</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(&lt;s&gt;,)</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(dog,)</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(this,)</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(i,)</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(a,)</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(is,)</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           i    a   is  like  dog  this  cat  <e>  <unk>\n",
       "(like,)  0.0  2.0  0.0   0.0  0.0   0.0  0.0  0.0    0.0\n",
       "(cat,)   0.0  0.0  0.0   0.0  0.0   0.0  0.0  2.0    0.0\n",
       "(<s>,)   1.0  0.0  0.0   0.0  0.0   1.0  0.0  0.0    0.0\n",
       "(dog,)   0.0  0.0  1.0   0.0  0.0   0.0  0.0  0.0    0.0\n",
       "(this,)  0.0  0.0  0.0   0.0  1.0   0.0  0.0  0.0    0.0\n",
       "(i,)     0.0  0.0  0.0   1.0  0.0   0.0  0.0  0.0    0.0\n",
       "(a,)     0.0  0.0  0.0   0.0  0.0   0.0  2.0  0.0    0.0\n",
       "(is,)    0.0  0.0  0.0   1.0  0.0   0.0  0.0  0.0    0.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sentences = [['i', 'like', 'a', 'cat'],\n",
    "                 ['this', 'dog', 'is', 'like', 'a', 'cat']]\n",
    "unique_words = list(set(sentences[0] + sentences[1]))\n",
    "bigram_counts = count_n_grams(sentences, 2)\n",
    "\n",
    "print('bigram counts')\n",
    "display(make_count_matrix(bigram_counts, unique_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "trigram counts\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>i</th>\n",
       "      <th>a</th>\n",
       "      <th>is</th>\n",
       "      <th>like</th>\n",
       "      <th>dog</th>\n",
       "      <th>this</th>\n",
       "      <th>cat</th>\n",
       "      <th>&lt;e&gt;</th>\n",
       "      <th>&lt;unk&gt;</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>(like, a)</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(dog, is)</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(this, dog)</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(&lt;s&gt;, &lt;s&gt;)</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(is, like)</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(i, like)</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(&lt;s&gt;, i)</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(a, cat)</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(&lt;s&gt;, this)</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               i    a   is  like  dog  this  cat  <e>  <unk>\n",
       "(like, a)    0.0  0.0  0.0   0.0  0.0   0.0  2.0  0.0    0.0\n",
       "(dog, is)    0.0  0.0  0.0   1.0  0.0   0.0  0.0  0.0    0.0\n",
       "(this, dog)  0.0  0.0  1.0   0.0  0.0   0.0  0.0  0.0    0.0\n",
       "(<s>, <s>)   1.0  0.0  0.0   0.0  0.0   1.0  0.0  0.0    0.0\n",
       "(is, like)   0.0  1.0  0.0   0.0  0.0   0.0  0.0  0.0    0.0\n",
       "(i, like)    0.0  1.0  0.0   0.0  0.0   0.0  0.0  0.0    0.0\n",
       "(<s>, i)     0.0  0.0  0.0   1.0  0.0   0.0  0.0  0.0    0.0\n",
       "(a, cat)     0.0  0.0  0.0   0.0  0.0   0.0  0.0  2.0    0.0\n",
       "(<s>, this)  0.0  0.0  0.0   0.0  1.0   0.0  0.0  0.0    0.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Show trigram counts\n",
    "print('\\ntrigram counts')\n",
    "trigram_counts = count_n_grams(sentences, 3)\n",
    "display(make_count_matrix(trigram_counts, unique_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calcualte probabilities\n",
    "def make_probability_matrix(n_plus1_gram_counts, vocabulary, k):\n",
    "    count_matrix = make_count_matrix(n_plus1_gram_counts, unique_words)\n",
    "    count_matrix += k  # smoothing \n",
    "    # vertically downwards across rows (axis 0)\n",
    "    # # running horizontally across columns (axis 1).\n",
    "    row_sum = count_matrix.sum(axis=1)  # sum of row\n",
    "    prob_matrix = count_matrix.div(row_sum, axis=0)\n",
    "    return prob_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bigram probabilities\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>i</th>\n",
       "      <th>a</th>\n",
       "      <th>is</th>\n",
       "      <th>like</th>\n",
       "      <th>dog</th>\n",
       "      <th>this</th>\n",
       "      <th>cat</th>\n",
       "      <th>&lt;e&gt;</th>\n",
       "      <th>&lt;unk&gt;</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>(like,)</th>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(cat,)</th>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.090909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(&lt;s&gt;,)</th>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(dog,)</th>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(this,)</th>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(i,)</th>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(a,)</th>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(is,)</th>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                i         a        is      like       dog      this       cat  \\\n",
       "(like,)  0.090909  0.272727  0.090909  0.090909  0.090909  0.090909  0.090909   \n",
       "(cat,)   0.090909  0.090909  0.090909  0.090909  0.090909  0.090909  0.090909   \n",
       "(<s>,)   0.181818  0.090909  0.090909  0.090909  0.090909  0.181818  0.090909   \n",
       "(dog,)   0.100000  0.100000  0.200000  0.100000  0.100000  0.100000  0.100000   \n",
       "(this,)  0.100000  0.100000  0.100000  0.100000  0.200000  0.100000  0.100000   \n",
       "(i,)     0.100000  0.100000  0.100000  0.200000  0.100000  0.100000  0.100000   \n",
       "(a,)     0.090909  0.090909  0.090909  0.090909  0.090909  0.090909  0.272727   \n",
       "(is,)    0.100000  0.100000  0.100000  0.200000  0.100000  0.100000  0.100000   \n",
       "\n",
       "              <e>     <unk>  \n",
       "(like,)  0.090909  0.090909  \n",
       "(cat,)   0.272727  0.090909  \n",
       "(<s>,)   0.090909  0.090909  \n",
       "(dog,)   0.100000  0.100000  \n",
       "(this,)  0.100000  0.100000  \n",
       "(i,)     0.100000  0.100000  \n",
       "(a,)     0.090909  0.090909  \n",
       "(is,)    0.100000  0.100000  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sentences = [['i', 'like', 'a', 'cat'],\n",
    "                 ['this', 'dog', 'is', 'like', 'a', 'cat']]\n",
    "unique_words = list(set(sentences[0] + sentences[1]))\n",
    "bigram_counts = count_n_grams(sentences, 2)\n",
    "print(\"bigram probabilities\")\n",
    "display(make_probability_matrix(bigram_counts, unique_words, k=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_perplexity(sentence, n_gram_dict, n_plus1_dict, vocabulary_size, k=1.0):\n",
    "    \"\"\"\n",
    "    Calculate perplexity for one sentence\n",
    "    \"\"\"\n",
    "    n = len(list(n_gram_dict.keys())[0])  # n = len of grams not list \n",
    "    sentence = [\"<s>\"] * n + sentence + [\"<e>\"]\n",
    "    sentence = tuple(sentence)\n",
    "    \n",
    "    N = len(sentence)\n",
    "    \n",
    "    product_pi = 1.0  # initialize the pi\n",
    "    for t in range(n, N):\n",
    "        n_gram = sentence[t-n:t]  # t-n to t \n",
    "        word = sentence[t]\n",
    "        \n",
    "        prob = estimate_probability(word, n_gram, n_gram_dict, n_plus1_dict,\n",
    "                                    vocabulary_size, k=k)\n",
    "        product_pi *= 1/prob\n",
    "    \n",
    "    perplexity = product_pi**(1/N)\n",
    "    \n",
    "    return perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity for first train sample: 2.8040\n",
      "Perplexity for test sample: 3.9654\n"
     ]
    }
   ],
   "source": [
    "# test your code\n",
    "\n",
    "sentences = [['i', 'like', 'a', 'cat'],\n",
    "                 ['this', 'dog', 'is', 'like', 'a', 'cat']]\n",
    "unique_words = list(set(sentences[0] + sentences[1]))\n",
    "\n",
    "unigram_counts = count_n_grams(sentences, 1)\n",
    "bigram_counts = count_n_grams(sentences, 2)\n",
    "\n",
    "\n",
    "perplexity_train1 = calculate_perplexity(sentences[0],\n",
    "                                         unigram_counts, bigram_counts,\n",
    "                                         len(unique_words), k=1.0)\n",
    "print(f\"Perplexity for first train sample: {perplexity_train1:.4f}\")\n",
    "\n",
    "test_sentence = ['i', 'like', 'a', 'dog']\n",
    "perplexity_test = calculate_perplexity(test_sentence,\n",
    "                                       unigram_counts, bigram_counts,\n",
    "                                       len(unique_words), k=1.0)\n",
    "print(f\"Perplexity for test sample: {perplexity_test:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def suggest_a_word(previous_token, n_gram_dict, n_plus1_gram_dict, vocabulary, k=1.0, start_with=None):\n",
    "    n = len(list(n_gram_dict.keys())[0])\n",
    "    # previous token can be shorter or longer than n_gram\n",
    "    previous_n_gram = previous_token[-n:]\n",
    "    \n",
    "    probabilities = estimate_probabilities(previous_n_gram, n_gram_dict, n_plus1_gram_dict,\n",
    "                                           vocabulary, k=k)\n",
    "    suggestion = None\n",
    "    \n",
    "    max_prob = 0\n",
    "    \n",
    "    for word, prob in probabilities.items():\n",
    "        # following up word starting with 'c' or something else\n",
    "        if start_with != None:\n",
    "            if not word.startswith(start_with):\n",
    "                continue\n",
    "            \n",
    "        if prob > max_prob:\n",
    "            suggestion = word\n",
    "            max_prob = prob\n",
    "            \n",
    "    return suggestion, max_prob\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The previous words are 'i like',\n",
      "\tand the suggested word is `a` with a probability of 0.2727\n",
      "\n",
      "The previous words are 'i like',\n",
      "\tand the suggested word is `cat` with a probability of 0.0909 when we specifying the staring character\n"
     ]
    }
   ],
   "source": [
    "# test your code\n",
    "sentences = [['i', 'like', 'a', 'cat'],\n",
    "             ['this', 'dog', 'is', 'like', 'a', 'cat']]\n",
    "unique_words = list(set(sentences[0] + sentences[1]))\n",
    "\n",
    "unigram_counts = count_n_grams(sentences, 1)\n",
    "bigram_counts = count_n_grams(sentences, 2)\n",
    "\n",
    "previous_tokens = [\"i\", \"like\"]\n",
    "tmp_suggest1 = suggest_a_word(previous_tokens, unigram_counts, bigram_counts, unique_words, k=1.0)\n",
    "print(f\"The previous words are 'i like',\\n\\tand the suggested word is `{tmp_suggest1[0]}` with a probability of {tmp_suggest1[1]:.4f}\")\n",
    "\n",
    "print()\n",
    "# test your code when setting the starts_with\n",
    "tmp_starts_with = 'c'\n",
    "tmp_suggest2 = suggest_a_word(previous_tokens, unigram_counts, bigram_counts, unique_words, k=1.0, start_with=tmp_starts_with)\n",
    "print(f\"The previous words are 'i like',\\n\\tand the suggested word is `{tmp_suggest2[0]}` with a probability of {tmp_suggest2[1]:.4f}\",\n",
    "      \"when we specifying the staring character\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# suggest based on several models\n",
    "def get_suggestions(previous_tokens, n_gram_counts_list, vocabulary, k=1.0, start_with=None):\n",
    "    model_counts = len(n_gram_counts_list)\n",
    "    suggestions = []\n",
    "    for i in range(model_counts-1):\n",
    "        n_gram_counts = n_gram_counts_list[i]\n",
    "        n_plus1_gram_counts = n_gram_counts_list[i+1]\n",
    "        \n",
    "        suggestion = suggest_a_word(previous_tokens, n_gram_counts,\n",
    "                                    n_plus1_gram_counts, vocabulary,\n",
    "                                    k=k, start_with=start_with)\n",
    "        suggestions.append(suggestion)\n",
    "    return suggestions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The previous words are 'i like', the suggestions are:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('a', 0.2727272727272727),\n",
       " ('a', 0.2),\n",
       " ('is', 0.1111111111111111),\n",
       " ('is', 0.1111111111111111)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# test your code\n",
    "sentences = [['i', 'like', 'a', 'cat'],\n",
    "             ['this', 'dog', 'is', 'like', 'a', 'cat']]\n",
    "unique_words = list(set(sentences[0] + sentences[1]))\n",
    "\n",
    "unigram_counts = count_n_grams(sentences, 1)\n",
    "bigram_counts = count_n_grams(sentences, 2)\n",
    "trigram_counts = count_n_grams(sentences, 3)\n",
    "quadgram_counts = count_n_grams(sentences, 4)\n",
    "qintgram_counts = count_n_grams(sentences, 5)\n",
    "\n",
    "n_gram_counts_list = [unigram_counts, bigram_counts, trigram_counts, quadgram_counts, qintgram_counts]\n",
    "previous_tokens = [\"i\", \"like\"]\n",
    "tmp_suggest3 = get_suggestions(previous_tokens, n_gram_counts_list, unique_words, k=1.0)\n",
    "\n",
    "print(f\"The previous words are 'i like', the suggestions are:\")\n",
    "display(tmp_suggest3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "553d5a3fbef48ee26961f0f15c5c9b1fe8a609c0888dfd8bcee652ae34bf17c4"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
