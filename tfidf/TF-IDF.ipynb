{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF: Term Frequency–Inverse Document Frequency\n",
    "\n",
    "TF-IDF is a numerical statistic that is intended to reflect how important \n",
    "a word is to a document in a collection or corpus. It is often used as a \n",
    "weighting factor in searches of information retrieval, \n",
    "text mining, and user modeling. The tf–idf value increases proportionally to \n",
    "the number of times a word appears in the document and \n",
    "is offset by the number of documents in the corpus \n",
    "that contain the word, which helps to adjust for the fact that some words \n",
    "appear more frequently in general. TF-IDF is one of \n",
    "the most popular term-weighting schemes today. A survey conducted \n",
    "in 2015 showed that 83% of text-based recommender systems in digital \n",
    "libraries use tf–idf."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this post, we will:\n",
    "\n",
    "- learn how to build up a tf-idf model from scratch\n",
    "- apply it to match names (entities) \n",
    "\n",
    "TF-IDF was invented by [Karen Jones](https://en.wikipedia.org/wiki/Karen_Sp%C3%A4rck_Jones),\n",
    "her original papers are among the most cited papers in the field of CIS."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definition\n",
    "\n",
    "- TF: term frequency is the count of a token present in a sentence\n",
    "- IDF: inverse document frequency is a weight indicating how commonly a \n",
    "word is used. The more frequency its usage across documents, the lower its score.\n",
    "For instance, 'the' shows up very often, it has a low score of IDF, The lower\n",
    "the score, the less important the world becomes.\n",
    "\n",
    "The formula to compute the tf-idf for a token $t$ of a document $d$\n",
    "in a document set is:\n",
    "\n",
    "$$tf-idf = tf(t, d) \\times idf(t), \\quad idf(t) = \\log(\\frac{n}{df(t)+1})$$\n",
    "\n",
    "where $n$ is the total number of documents in the document sett and \n",
    "$df(t)$ is the _document frequency_ of $t$; the document frequency is\n",
    "the number of documents in the document set that contains the term $t$.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the IDF, we will implement the smooth version based on the document of \n",
    "`sklearn`, which is \n",
    "\n",
    "$$idf(t) = \\log \\left ( \\frac{1+n}{1+df(t)} \\right ) +1 $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import struct\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mplt\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.preprocessing import normalize\n",
    "%config InlineBackend.figure_formats = ['svg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['this is the first document',\n",
       "       'this document is the second document',\n",
       "       'and this is the third one', 'is this the first document'],\n",
       "      dtype='<U36')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# document example a list\n",
    "corpus_1 = [\n",
    "     'this is the first document',\n",
    "     'this document is the second document',\n",
    "     'and this is the third one',\n",
    "     'is this the first document',\n",
    "]\n",
    "corpus_1 = np.array(corpus_1)\n",
    "corpus_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_dimension(corpus: np.ndarray) -> dict:\n",
    "     \"\"\"\n",
    "     Count the dimension of token based on unique tokens in the corpus\n",
    "     \n",
    "     Parameters\n",
    "     ----------\n",
    "     corpus: np.ndarray, shape (n, ) or (n, 1), where each row is a \n",
    "     a long string with spaces such as \"this is the first document\"\n",
    "     \n",
    "     Returns\n",
    "     ----------\n",
    "     unique_tokens_dict: dict{'token': dimension: int}\n",
    "     \"\"\"\n",
    "     corpus = corpus.ravel()  # make it shape (n, )\n",
    "     \n",
    "     unique_tokens = []\n",
    "     \n",
    "     # loop over rows of corpus np.ndarray\n",
    "     for row in corpus:\n",
    "          # split token \n",
    "          # you might need different loop based on different input\n",
    "          # here we are dealing with a long string with spaces\n",
    "          # your row input might be array or list of token \n",
    "          for token in row.split():  # split based on space \n",
    "               # ignore alphabet as len('b') = 1\n",
    "               if len(token) >= 2 and token not in unique_tokens:\n",
    "                    unique_tokens.append(token)\n",
    "          # sort it \n",
    "          unique_tokens.sort()\n",
    "          \n",
    "          unique_tokens_dict = {token:i for i, token in enumerate(unique_tokens)}\n",
    "          \n",
    "     return unique_tokens_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'and': 0,\n",
       " 'document': 1,\n",
       " 'first': 2,\n",
       " 'is': 3,\n",
       " 'one': 4,\n",
       " 'second': 5,\n",
       " 'the': 6,\n",
       " 'third': 7,\n",
       " 'this': 8}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_dimension(corpus_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_word(corpus: np.ndarray, word: str) -> int:\n",
    "    \"\"\"\n",
    "    Count word frequency from the corpus\n",
    "    \n",
    "    Parameters\n",
    "    -----------\n",
    "    corpus: np.ndarray, shape (n, ) or (n, 1)\n",
    "    word: string\n",
    "    \n",
    "    Return\n",
    "    ------------\n",
    "    count: integer, word frequency\n",
    "    \"\"\"\n",
    "    corpus = corpus.ravel()\n",
    "    count = 0\n",
    "    \n",
    "    for row in corpus:\n",
    "        # assumes row is string \n",
    "        if word in row:\n",
    "            count += 1\n",
    "    \n",
    "    return count "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two functions above are very easy to understand. We will use them to \n",
    "calculate the term frequency. Before we proceed, let's recap:\n",
    "\n",
    "- we have documents\n",
    "- we have texts in each document \n",
    "- we have a set of unique words in the texts of each document\n",
    "- we have a set of unique words in all the documents \n",
    "\n",
    "To calculate the inverse document frequency (IDF), we will use the following\n",
    "formula:\n",
    "\n",
    "$$idf(t) = \\log \\left ( \\frac{1+n}{1+df(t)} \\right ) +1 $$\n",
    "\n",
    "- $n$ is the total number of documents\n",
    "- $df(t)$ is the number of documents containing term $t$\n",
    "\n",
    "To help us to "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_idf_transform(corpus: np.ndarray, token_dimenstion: dict):\n",
    "    \"\"\"\n",
    "    Transform the corpus into a sparse matrix \n",
    "    \"\"\"\n",
    "    \n",
    "    corpus = corpus.ravel()\n",
    "    \n",
    "    rows = []\n",
    "    columns = []\n",
    "    frequency = []\n",
    "    tf_val = []\n",
    "    idf_val = []\n",
    "    \n",
    "    for idx, row in enumerate(corpus):\n",
    "        word_freq = dict(Counter(row.split()))\n",
    "        \n",
    "    for word, freq in word_freq.items():\n",
    "        # only retrieve those words with len >= 2\n",
    "        if len(word) >= 2:\n",
    "            # get the dimension as idx\n",
    "            # if the key does not exist, dimension_idx = -1\n",
    "            word_dimension_idx = token_dimenstion.get(word, -1)\n",
    "            \n",
    "            if word_dimension_idx != -1:\n",
    "                # first, store the index of the document\n",
    "                rows.append(idx)\n",
    "                # second, store the index of the word\n",
    "                columns.append(word_dimension_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'this': 1, 'is': 1, 'the': 1, 'first': 1, 'document': 1})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(corpus_1[0].split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlpvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3e3b0cbab2df3678c4f1a03da0a28e52ab0c4892f8b9cf10129105868fe79134"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
